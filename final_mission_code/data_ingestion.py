#!/usr/bin/env python3
"""
ë°ì´í„° ì¸ì œìŠ¤ì²œ íŒŒì´í”„ë¼ì¸
PostgreSQL/PGVectorì— ë¬¸ì„œë¥¼ ë¡œë”©í•˜ê³  ë²¡í„°í™”í•˜ëŠ” ë³„ë„ ìŠ¤í¬ë¦½íŠ¸
"""

import os
from dotenv import load_dotenv
from langchain_upstage import UpstageDocumentParseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_postgres import PGVector

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# API í‚¤ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ê¸°)
os.environ["UPSTAGE_API_KEY"] = os.getenv("UPSTAGE_API_KEY", "")

# PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ê¸°)
DATABASE_URL = os.getenv("DATABASE_URL")
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "documents")


def load_and_process_documents(file_path: str = "./test_modified.pdf"):
    """ë¬¸ì„œ ë¡œë”© ë° ì²˜ë¦¬"""
    print("ğŸ“„ ë¬¸ì„œ ë¡œë”© ì¤‘...")
    
    # Document loading
    loader = UpstageDocumentParseLoader(
        file_path,
        split="page",
        output_format="markdown",
        ocr="auto",
        coordinates=True,
    )
    docs = loader.load()
    print(f"âœ… ë¬¸ì„œ ë¡œë”© ì™„ë£Œ: {len(docs)}ê°œ í˜ì´ì§€")

    # Document chunking
    print("ğŸ”ª ë¬¸ì„œ ì²­í‚¹ ì¤‘...")
    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)
    docs_splitter = splitter.split_documents(docs)
    print(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(docs_splitter)}ê°œ ì²­í¬")

    return docs_splitter


def initialize_embeddings():
    """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
    print("ğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
    
    hf_embeddings = HuggingFaceEmbeddings(
        model_name="intfloat/multilingual-e5-large-instruct",
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True},
    )
    
    print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    return hf_embeddings


def create_vector_store(docs_splitter, embeddings):
    """ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ë° ë¬¸ì„œ ì €ì¥"""
    print("ğŸ—„ï¸ PostgreSQL/PGVector ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
    
    try:
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì—°ê²°
        vectorstore = PGVector(
            embeddings=embeddings,
            connection_string=DATABASE_URL,
            collection_name=COLLECTION_NAME,
        )
        
        # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ì„ íƒì‚¬í•­)
        print("ğŸ—‘ï¸ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì¤‘...")
        vectorstore.delete_collection()
        
        # ìƒˆë¡œìš´ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        vectorstore = PGVector.from_documents(
            documents=docs_splitter,
            embedding=embeddings,
            connection_string=DATABASE_URL,
            collection_name=COLLECTION_NAME,
        )
        
        print("âœ… ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ")
        return vectorstore
        
    except Exception as e:
        print(f"âŒ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨: {e}")
        raise


def main():
    """ë©”ì¸ ë°ì´í„° ì¸ì œìŠ¤ì²œ ì‹¤í–‰"""
    try:
        print("ğŸš€ ë°ì´í„° ì¸ì œìŠ¤ì²œ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print(f"ğŸ“ ë°ì´í„°ë² ì´ìŠ¤: {DATABASE_URL}")
        print(f"ğŸ“ ì»¬ë ‰ì…˜: {COLLECTION_NAME}")
        print("-" * 50)
        
        # 1. ë¬¸ì„œ ë¡œë”© ë° ì²˜ë¦¬
        docs_splitter = load_and_process_documents()
        
        # 2. ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        embeddings = initialize_embeddings()
        
        # 3. ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ë° ë°ì´í„° ì €ì¥
        vectorstore = create_vector_store(docs_splitter, embeddings)
        
        # 4. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        print("ğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì¤‘...")
        retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
        test_results = retriever.invoke("ì•„ì£¼ëŒ€í•™êµ")
        print(f"âœ… ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {len(test_results)}ê°œ ê²°ê³¼")
        
        print("-" * 50)
        print("ğŸ‰ ë°ì´í„° ì¸ì œìŠ¤ì²œ ì™„ë£Œ!")
        print("ì´ì œ RAG ì„œë¹„ìŠ¤ì—ì„œ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ì¸ì œìŠ¤ì²œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()