from typing import List, Dict, Any, Optional, AsyncGenerator, Literal
from typing_extensions import TypedDict
import asyncio
import logging
import enum
import uuid
import os
from datetime import datetime

# Jupyter ì½”ë“œì—ì„œ ì‚¬ìš©í•œ ë™ì¼í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
from langchain_anthropic import ChatAnthropic
from langchain.prompts import ChatPromptTemplate
from langchain_core.messages import AIMessage, HumanMessage, BaseMessage
from langchain_core.output_parsers import StrOutputParser
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

# RAG ì»´í¬ë„ŒíŠ¸ë“¤
from langchain_upstage import UpstageDocumentParseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from pydantic import BaseModel, Field

# ëž­í“¨ì¦ˆ
from langfuse import Langfuse, get_client
from langfuse.langchain import CallbackHandler

# ìŠ¤íŠ¸ë¦¬ë°
import asyncio
from langchain_core.output_parsers import StrOutputParser

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# .env íŒŒì¼ ë¡œë“œ (í•„ìš”í•œ ê²½ìš°)
load_dotenv()




# Jupyter ì½”ë“œì—ì„œ ê°€ì ¸ì˜¨ ëª¨ë¸ë“¤
class Category(enum.Enum):
    """ì¿¼ë¦¬ ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬"""

    DOCUMENT = "document"
    GENERAL = "general"
    GREETING = "greeting"


class QueryClassification(BaseModel):
    """ì‚¬ìš©ìž ì¿¼ë¦¬ ë¶„ë¥˜ ëª¨ë¸ - Jupyter ì½”ë“œì™€ ë™ì¼"""

    category: Category = Field(
        description="ì¿¼ë¦¬ë¥¼ ì¹´í…Œê³ ë¦¬í™” í•˜ì„¸ìš”. DOCUMENT(ë¬¸ì„œ ê´€ë ¨ ì§ˆë¬¸) / GENERAL(ì¼ë°˜ì ì¸ ì§ˆë¬¸) / GREETING(ì¸ì‚¬) ì¤‘ì— í•˜ë‚˜ë¡œ êµ¬ë¶„í•˜ì„¸ìš”."
    )
    reasoning: str = Field(description="ì™œ ì´ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí–ˆëŠ”ì§€ ì„¤ëª…í•˜ì„¸ìš”.")


class State(TypedDict):
    """LangGraph ìƒíƒœ - Jupyter ì½”ë“œì™€ ë™ì¼"""

    messages: List[BaseMessage]
    context: List[str]
    category: Optional[str]


class RAGService:
    """
    Jupyter ë…¸íŠ¸ë¶ì˜ RAG ë¡œì§ì„ ì„œë¹„ìŠ¤ í´ëž˜ìŠ¤ë¡œ ìº¡ìŠí™”í•œ ê²ƒìž…ë‹ˆë‹¤.

    ì´ í´ëž˜ìŠ¤ëŠ” ì›ë³¸ ì½”ë“œì˜ ëª¨ë“  ê¸°ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œ,
    ì—¬ëŸ¬ ì„¸ì…˜ì„ ë™ì‹œì— ì²˜ë¦¬í•  ìˆ˜ ìžˆë„ë¡ í™•ìž¥í–ˆìŠµë‹ˆë‹¤.
    """

    def __init__(self):
        """RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        self.retriever = None
        self.llm = None
        self.graph = None
        self.conversation_histories: Dict[str, List[BaseMessage]] = {}
        self.is_initialized = False

    async def initialize(self, file_path: str = "./test_modified.pdf"):
        """
        RAG ì»´í¬ë„ŒíŠ¸ë“¤ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        ì´ í•¨ìˆ˜ëŠ” Jupyter ì½”ë“œì˜ initialize_rag_components()ì™€ ë™ì¼í•œ ë¡œì§ìž…ë‹ˆë‹¤.
        í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ê³ , ì´í›„ ëª¨ë“  ìš”ì²­ì—ì„œ ìž¬ì‚¬ìš©ë©ë‹ˆë‹¤.
        """

        try:
            logger.info("RAG ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹œìž‘...")

            # ë¬¸ì„œ ë¡œë”© - Jupyter ì½”ë“œì™€ ë™ì¼
            logger.info("ë¬¸ì„œ ë¡œë”© ì¤‘...")
            loader = UpstageDocumentParseLoader(
                file_path,
                split="page",
                output_format="markdown",
                ocr="auto",
                coordinates=True,
            )
            docs = loader.load()
            logger.info(f"ë¬¸ì„œ ë¡œë”© ì™„ë£Œ: {len(docs)} íŽ˜ì´ì§€")

            # ë¬¸ì„œ ì²­í‚¹ - Jupyter ì½”ë“œì™€ ë™ì¼
            logger.info("ë¬¸ì„œ ì²­í‚¹ ì¤‘...")
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=1500, chunk_overlap=300
            )
            docs_splitter = splitter.split_documents(docs)
            logger.info(f"ì²­í‚¹ ì™„ë£Œ: {len(docs_splitter)} ì²­í¬")

            # ìž„ë² ë”© ëª¨ë¸ - Jupyter ì½”ë“œì™€ ë™ì¼í•œ ë¡œì§
            logger.info("ìž„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
            device = self._get_device()
            logger.info(f"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")

            hf_embeddings = HuggingFaceEmbeddings(
                model_name="intfloat/multilingual-e5-large-instruct",
                model_kwargs={"device": device},
                encode_kwargs={"normalize_embeddings": True},
            )

            # ë²¡í„° ìŠ¤í† ì–´ - Jupyter ì½”ë“œì™€ ë™ì¼
            logger.info("ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
            vectorstore = FAISS.from_documents(
                documents=docs_splitter,
                embedding=hf_embeddings,
            )
            logger.info("ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ")

            # ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •
            self.retriever = vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 5},
            )

            # LLM ì´ˆê¸°í™” - Jupyter ì½”ë“œì™€ ë™ì¼í•˜ê²Œ ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
            logger.info("LLM ì´ˆê¸°í™” ì¤‘...")
            self.llm = ChatAnthropic(
                model="claude-3-haiku-20240307",
                temperature=0,
                streaming=True,  # ðŸ”¥ ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
            )

            # LangGraph ì´ˆê¸°í™”
            self._build_graph()

            self.is_initialized = True
            logger.info("RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ!")

        except Exception as e:
            logger.error(f"ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")
            raise

    def _get_device(self) -> str:
        """ë””ë°”ì´ìŠ¤ ì„ íƒ - Jupyter ì½”ë“œì™€ ë™ì¼í•œ ë¡œì§"""
        device = "cpu"
        try:
            import torch

            if torch.cuda.is_available():
                device = "cuda"
            elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
                device = "mps"
        except ImportError:
            pass
        return device

    def _build_graph(self):

        # StateGraph ìƒì„±
        graph_builder = StateGraph(State)

        # ë…¸ë“œë“¤ ì¶”ê°€
        graph_builder.add_node("router", self._router)
        graph_builder.add_node("retrieve", self._retrieve_documents)
        graph_builder.add_node("document_qa", self._document_qa)
        graph_builder.add_node("general_qa", self._general_qa)
        graph_builder.add_node("greeting", self._greeting)

        # ì—£ì§€ ì—°ê²° 
        graph_builder.add_edge(START, "router")
        graph_builder.add_conditional_edges(
            "router",
            self._route_by_category,
            {
                "document_qa": "retrieve",
                "general_qa": "general_qa",
                "greeting": "greeting",
            },
        )
        graph_builder.add_edge("retrieve", "document_qa")
        graph_builder.add_edge("document_qa", END)
        graph_builder.add_edge("general_qa", END)
        graph_builder.add_edge("greeting", END)

        # ê·¸ëž˜í”„ ì»´íŒŒì¼
        graph = graph_builder.compile().with_config(
            config={"callbacks": [langfuse_handler]}
        )
        print("LangGraph êµ¬ì„± ì™„ë£Œ!")


    def _format_docs(self, docs) -> str:
        """ë¬¸ì„œ í¬ë§·íŒ… - Jupyter ì½”ë“œì™€ ë™ì¼"""
        return "\\n\\n".join(doc.page_content for doc in docs)

    def _router(self, state: State) -> Dict[str, Any]:
        """
        ì¿¼ë¦¬ ë¶„ë¥˜ ë¼ìš°í„° - Jupyter ì½”ë“œì™€ ë™ì¼í•œ ë¡œì§

        ì´ í•¨ìˆ˜ëŠ” ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì„ ë¶„ì„í•´ì„œ ì–´ë–¤ ìœ í˜•ì˜ ì§ˆë¬¸ì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤.
        """

        logger.info("ì¿¼ë¦¬ ë¶„ë¥˜ ì¤‘...")

        # ê°€ìž¥ ìµœê·¼ ì‚¬ìš©ìž ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°
        user_message = state["messages"][-1].content

        # ë¼ìš°í„° ìž…ë ¥ ìƒì„± - Jupyter ì½”ë“œì™€ ë™ì¼í•œ í”„ë¡¬í”„íŠ¸
        router_input = f"""
        ë‹¤ìŒ ì‚¬ìš©ìž ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ê³  ì¹´í…Œê³ ë¦¬ë¥¼ ê²°ì •í•˜ì„¸ìš”.
        ì¹´í…Œê³ ë¦¬:
        - document: ë¬¸ì„œ ë‚´ìš©ì— ê´€í•œ ì§ˆë¬¸ (ì˜ˆ: "ì•„ì£¼ëŒ€í•™êµì— ëŒ€í•´ ì•Œë ¤ì¤˜", "ì´ ë¬¸ì„œì—ì„œ ì¤‘ìš”í•œ ë‚´ìš©ì€?")
        - general: ì¼ë°˜ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ, ë¬¸ì„œì™€ ê´€ë ¨ì´ ì—†ìŒ (ì˜ˆ: "ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?", "íŒŒì´ì¬ì´ëž€?")
        - greeting: ì¸ì‚¬ë§ (ì˜ˆ: "ì•ˆë…•", "ë°˜ê°€ì›Œ", "ë­í•´?")
        
        ì¿¼ë¦¬: {user_message}
        """

        # êµ¬ì¡°í™”ëœ ì¶œë ¥ìœ¼ë¡œ ë¶„ë¥˜
        structured_llm = self.llm.with_structured_output(QueryClassification)
        classification = structured_llm.invoke(router_input)

        category = classification.category.value
        logger.info(f"ë¶„ë¥˜ ê²°ê³¼: {category} (ì´ìœ : {classification.reasoning})")

        return {"category": category}

    def _route_by_category(
        self, state: State
    ) -> Literal["document_qa", "general_qa", "greeting"]:
        """ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ë¼ìš°íŒ… - Jupyter ì½”ë“œì™€ ë™ì¼"""
        category = state.get("category", "").lower()

        if category == "document":
            return "document_qa"
        elif category == "general":
            return "general_qa"
        elif category == "greeting":
            return "greeting"
        else:
            return "general_qa"

    def _retrieve_documents(self, state: State) -> Dict[str, Any]:
        """ë¬¸ì„œ ê²€ìƒ‰ - Jupyter ì½”ë“œì™€ ë™ì¼í•œ ë¡œì§"""
        logger.info("ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")

        # ê°€ìž¥ ìµœê·¼ ì‚¬ìš©ìž ë©”ì‹œì§€
        user_message = state["messages"][-1]

        # ë¬¸ì„œ ê²€ìƒ‰
        docs = self.retriever.invoke(user_message.content)

        # ë¬¸ì„œ í¬ë§·íŒ…
        formatted_docs = self._format_docs(docs)
        logger.info(f"ê²€ìƒ‰ ì™„ë£Œ: {len(docs)} ë¬¸ì„œ ì°¾ìŒ")

        return {"context": [formatted_docs]}

    async def _stream_llm_response(
        self, llm, formatted_message
    ) -> AsyncGenerator[str, None]:
        """
        LLM ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ìƒì„±í•˜ëŠ” í•¨ìˆ˜

        ì´ í•¨ìˆ˜ê°€ í•µì‹¬ìž…ë‹ˆë‹¤. Jupyter ì½”ë“œì˜ stream_llm_responseë¥¼
        AsyncGeneratorë¡œ ë³€í™˜í•´ì„œ FastAPIì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìžˆê²Œ í–ˆìŠµë‹ˆë‹¤.

        ì£¼ì˜: async generatorì—ì„œëŠ” return valueë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
        ëŒ€ì‹  yieldë¥¼ ì‚¬ìš©í•´ì„œ ê°’ë“¤ì„ í•˜ë‚˜ì”© ìƒì„±í•©ë‹ˆë‹¤.
        """

        # ì²´ì¸ ìƒì„± - Jupyter ì½”ë“œì™€ ë™ì¼
        chain = llm | StrOutputParser()

        # ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
        async for chunk in chain.astream(formatted_message):
            if chunk:
                # ê° ì²­í¬ë¥¼ yield - ì´ê²ƒì´ HTTP ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì „ì†¡ë©ë‹ˆë‹¤
                yield chunk

        # async generatorì—ì„œëŠ” return ëŒ€ì‹  ê·¸ëƒ¥ í•¨ìˆ˜ê°€ ëë‚˜ë©´ ë©ë‹ˆë‹¤

    async def _document_qa(self, state: State) -> Dict[str, Any]:
        """
        ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ - Jupyter ì½”ë“œì˜ ìŠ¤íŠ¸ë¦¬ë° ë¡œì§ì„ ê·¸ëŒ€ë¡œ ìœ ì§€

        ì´ í•¨ìˆ˜ì—ì„œ ì‹¤ì œ ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
        ì›ë³¸ ì½”ë“œì˜ í”„ë¡¬í”„íŠ¸ì™€ ë¡œì§ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

        ìˆ˜ì •ëœ ë¶€ë¶„: ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ëˆ„ì í•´ì„œ ìµœì¢… AIMessageë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """

        logger.info("ë¬¸ì„œ ê¸°ë°˜ ì‘ë‹µ ìƒì„± ì¤‘...")

        context = state["context"][0] if state["context"] else "ë¬¸ì„œ ì •ë³´ ì—†ìŒ"
        user_message = state["messages"][-1].content

        # ì´ì „ ëŒ€í™” ížˆìŠ¤í† ë¦¬ êµ¬ì„± - Jupyter ì½”ë“œì™€ ë™ì¼
        history_messages = state["messages"][:-1]
        formatted_history = ""
        for msg in history_messages:
            role = "ì‚¬ìš©ìž" if isinstance(msg, HumanMessage) else "AI"
            formatted_history += f"{role}: {msg.content}\\n"

        # í”„ë¡¬í”„íŠ¸ ìƒì„± - Jupyter ì½”ë“œì™€ ë™ì¼í•œ í”„ë¡¬í”„íŠ¸
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """ë„ˆëŠ” ì¹œì ˆí•œ í•œêµ­ì–´ AI ë¹„ì„œì•¼. 
        ì œê³µëœ ë¬¸ì„œ ë‚´ìš©(context)ê³¼ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µí•´.
        ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ëŒ€ë‹µí•˜ê³ , ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ëŒ€ë‹µí•˜ì§€ ë§ê³  ëª¨ë¥¸ë‹¤ê³  í•´.
        
        ì°¸ê³  ë¬¸ì„œ:
        {context}
        
        ì´ì „ ëŒ€í™”:
        {chat_history}
        """,
                ),
                ("user", "{user_input}"),
            ]
        )

        formatted_message = prompt.format_messages(
            context=context, chat_history=formatted_history, user_input=user_message
        )

        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ëˆ„ì í•˜ì—¬ ì „ì²´ ì‘ë‹µ ìƒì„±
        full_response = ""
        async for chunk in self._stream_llm_response(self.llm, formatted_message):
            full_response += chunk

        logger.info("ë¬¸ì„œ ê¸°ë°˜ ì‘ë‹µ ìƒì„± ì™„ë£Œ")
        return {"messages": [AIMessage(content=full_response)]}

    async def _general_qa(self, state: State) -> Dict[str, Any]:
        """ì¼ë°˜ ì§ˆì˜ì‘ë‹µ - Jupyter ì½”ë“œì™€ ë™ì¼í•œ ë¡œì§"""
        logger.info("ì¼ë°˜ ì‘ë‹µ ìƒì„± ì¤‘...")

        user_message = state["messages"][-1].content

        # ì´ì „ ëŒ€í™” ížˆìŠ¤í† ë¦¬ êµ¬ì„±
        history_messages = state["messages"][:-1]
        formatted_history = ""
        for msg in history_messages:
            role = "ì‚¬ìš©ìž" if isinstance(msg, HumanMessage) else "AI"
            formatted_history += f"{role}: {msg.content}\\n"

        # í”„ë¡¬í”„íŠ¸ ìƒì„± - Jupyter ì½”ë“œì™€ ë™ì¼
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """ë„ˆëŠ” ì¹œì ˆí•œ í•œêµ­ì–´ AI ë¹„ì„œì•¼. 
        ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´. í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•´.
        
        ì´ì „ ëŒ€í™”:
        {chat_history}
        """,
                ),
                ("user", "{user_input}"),
            ]
        )

        formatted_message = prompt.format_messages(
            user_input=user_message, chat_history=formatted_history
        )

        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ëˆ„ì í•´ì„œ ì „ì²´ ì‘ë‹µ ìƒì„±
        full_response = ""
        async for chunk in self._stream_llm_response(self.llm, formatted_message):
            full_response += chunk

        logger.info("ì¼ë°˜ ì‘ë‹µ ìƒì„± ì™„ë£Œ")
        return {"messages": [AIMessage(content=full_response)]}

    async def _greeting(self, state: State) -> Dict[str, Any]:
        """ì¸ì‚¬ë§ ì‘ë‹µ - Jupyter ì½”ë“œì™€ ë™ì¼í•œ ë¡œì§"""
        logger.info("ì¸ì‚¬ ì‘ë‹µ ìƒì„± ì¤‘...")

        user_message = state["messages"][-1].content

        # ì´ì „ ëŒ€í™” ížˆìŠ¤í† ë¦¬ êµ¬ì„±
        history_messages = state["messages"][:-1]
        formatted_history = ""
        for msg in history_messages:
            role = "ì‚¬ìš©ìž" if isinstance(msg, HumanMessage) else "AI"
            formatted_history += f"{role}: {msg.content}\\n"

        # í”„ë¡¬í”„íŠ¸ ìƒì„± - Jupyter ì½”ë“œì™€ ë™ì¼
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """ë„ˆëŠ” ì¹œì ˆí•œ í•œêµ­ì–´ AI ë¹„ì„œì•¼.
        ì‚¬ìš©ìžì˜ ì¸ì‚¬ì— ì¹œê·¼í•˜ê³  ë”°ëœ»í•˜ê²Œ ì‘ë‹µí•´. ê°„ê²°í•˜ê²Œ í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•´.

        ì´ì „ ëŒ€í™”:
        {chat_history}
        """,
                ),
                ("user", "{user_input}"),
            ]
        )

        formatted_message = prompt.format_messages(
            user_input=user_message, chat_history=formatted_history
        )

        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ëˆ„ì í•´ì„œ ì „ì²´ ì‘ë‹µ ìƒì„±
        full_response = ""
        async for chunk in self._stream_llm_response(self.llm, formatted_message):
            full_response += chunk

        logger.info("ì¸ì‚¬ ì‘ë‹µ ìƒì„± ì™„ë£Œ")
        return {"messages": [AIMessage(content=full_response)]}

    async def stream_chat_response(
        self, message: str, session_id: Optional[str] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        ì§„ì •í•œ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
        ì˜ìƒì—ì„œ ë³´ì—¬ì£¼ëŠ” ê²ƒì²˜ëŸ¼ LLMì˜ ì‹¤ì œ í† í° ìŠ¤íŠ¸ë¦¬ë°ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
        """

        if not self.is_initialized:
            raise RuntimeError("RAG ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        # ì„¸ì…˜ ID ìƒì„± (ì œê³µë˜ì§€ ì•Šì€ ê²½ìš°)
        if not session_id:
            session_id = str(uuid.uuid4())

        # ëŒ€í™” ížˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ì´ˆê¸°í™”
        if session_id not in self.conversation_histories:
            self.conversation_histories[session_id] = []

        conversation_history = self.conversation_histories[session_id]

        try:
            # ì‚¬ìš©ìž ë©”ì‹œì§€ ì¶”ê°€
            user_message = HumanMessage(content=message)
            conversation_history.append(user_message)

            # ì´ˆê¸° ìƒíƒœ êµ¬ì„±
            initial_state = {
                "messages": conversation_history.copy(),
                "context": [],
                "category": None,
            }

            # ì‹œìž‘ ì•Œë¦¼
            yield {
                "type": "start",
                "content": "ìš”ì²­ ì²˜ë¦¬ë¥¼ ì‹œìž‘í•©ë‹ˆë‹¤...",
                "metadata": {"step": "start"},
            }

            # 1ë‹¨ê³„: ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
            yield {
                "type": "classification",
                "content": "ì¿¼ë¦¬ë¥¼ ë¶„ë¥˜í•˜ê³  ìžˆìŠµë‹ˆë‹¤...",
                "metadata": {"step": "router"},
            }

            router_result = self._router(initial_state)
            category = router_result["category"]
            initial_state.update(router_result)

            yield {
                "type": "classification_result",
                "content": f"ì¿¼ë¦¬ ë¶„ë¥˜ ì™„ë£Œ: {category}",
                "category": category,
                "metadata": {"step": "router_complete"},
            }

            # 2ë‹¨ê³„: ë¬¸ì„œ ê²€ìƒ‰ (í•„ìš”í•œ ê²½ìš°)
            if category == "document":
                yield {
                    "type": "retrieval",
                    "content": "ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  ìžˆìŠµë‹ˆë‹¤...",
                    "metadata": {"step": "retrieve"},
                }

                retrieve_result = self._retrieve_documents(initial_state)
                initial_state.update(retrieve_result)

                yield {
                    "type": "retrieval_result",
                    "content": "ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ",
                    "metadata": {
                        "step": "retrieve_complete",
                        "docs_found": len(initial_state["context"]),
                    },
                }

            # 3ë‹¨ê³„: ì‹¤ì‹œê°„ ì‘ë‹µ ìƒì„±
            yield {
                "type": "generation_start",
                "content": "ì‘ë‹µì„ ìƒì„±í•˜ê³  ìžˆìŠµë‹ˆë‹¤...",
                "metadata": {"step": "generation"},
            }

            # ðŸ”¥ í•µì‹¬: ì—¬ê¸°ì„œ ì‹¤ì œ LLM ìŠ¤íŠ¸ë¦¬ë°ì„ ë°”ë¡œ ì—°ê²°í•©ë‹ˆë‹¤
            full_response = ""

            # í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
            context = initial_state["context"][0] if initial_state["context"] else ""
            formatted_message = self._prepare_prompt(
                message, conversation_history[:-1], context, category
            )

            # ðŸ”¥ LLMì˜ ì‹¤ì œ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ì„ ì§ì ‘ yield
            chain = self.llm | StrOutputParser()

            async for chunk in chain.astream(formatted_message):
                if chunk:  # ë¹ˆ ì²­í¬ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ
                    full_response += chunk

                    # ê° í† í°ì„ ì¦‰ì‹œ ì „ì†¡ - ì´ê²ƒì´ ì§„ì§œ ìŠ¤íŠ¸ë¦¬ë°ìž…ë‹ˆë‹¤!
                    yield {
                        "type": "generation_chunk",
                        "content": chunk,  # ì‹¤ì œ LLM í† í°ì„ ê·¸ëŒ€ë¡œ ì „ì†¡
                        "category": category,
                        "metadata": {
                            "partial_response": full_response,
                            "tokens_so_far": len(full_response.split()),
                        },
                    }

            # AI ì‘ë‹µì„ ížˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            ai_message = AIMessage(content=full_response)
            conversation_history.append(ai_message)

            # ìµœì¢… ì‘ë‹µ
            yield {
                "type": "final",
                "content": full_response,
                "category": category,
                "metadata": {
                    "session_id": session_id,
                    "total_tokens": len(full_response.split()),
                    "conversation_length": len(conversation_history),
                },
                "is_final": True,
            }

        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            yield {
                "type": "error",
                "content": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "metadata": {"error_type": type(e).__name__},
                "is_final": True,
            }

    def _prepare_prompt(
        self, user_message: str, history: List[BaseMessage], context: str, category: str
    ):
        """ì¹´í…Œê³ ë¦¬ì— ë”°ë¼ ì ì ˆí•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤."""

        # ížˆìŠ¤í† ë¦¬ í¬ë§·íŒ…
        formatted_history = ""
        for msg in history:
            role = "ì‚¬ìš©ìž" if isinstance(msg, HumanMessage) else "AI"
            formatted_history += f"{role}: {msg.content}\\n"

        if category == "document":
            prompt = ChatPromptTemplate.from_messages(
                [
                    (
                        "system",
                        """ë„ˆëŠ” ì¹œì ˆí•œ í•œêµ­ì–´ AI ë¹„ì„œì•¼. 
            ì œê³µëœ ë¬¸ì„œ ë‚´ìš©(context)ê³¼ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µí•´.
            ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ëŒ€ë‹µí•˜ê³ , ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ëŒ€ë‹µí•˜ì§€ ë§ê³  ëª¨ë¥¸ë‹¤ê³  í•´.
            
            ì°¸ê³  ë¬¸ì„œ:
            {context}
            
            ì´ì „ ëŒ€í™”:
            {chat_history}
            """,
                    ),
                    ("user", "{user_input}"),
                ]
            )

            return prompt.format_messages(
                context=context, chat_history=formatted_history, user_input=user_message
            )

        elif category == "greeting":
            prompt = ChatPromptTemplate.from_messages(
                [
                    (
                        "system",
                        """ë„ˆëŠ” ì¹œì ˆí•œ í•œêµ­ì–´ AI ë¹„ì„œì•¼.
            ì‚¬ìš©ìžì˜ ì¸ì‚¬ì— ì¹œê·¼í•˜ê³  ë”°ëœ»í•˜ê²Œ ì‘ë‹µí•´. ê°„ê²°í•˜ê²Œ í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•´.

            ì´ì „ ëŒ€í™”:
            {chat_history}
            """,
                    ),
                    ("user", "{user_input}"),
                ]
            )

            return prompt.format_messages(
                user_input=user_message, chat_history=formatted_history
            )

        else:  # general
            prompt = ChatPromptTemplate.from_messages(
                [
                    (
                        "system",
                        """ë„ˆëŠ” ì¹œì ˆí•œ í•œêµ­ì–´ AI ë¹„ì„œì•¼. 
            ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´. í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•´.
            
            ì´ì „ ëŒ€í™”:
            {chat_history}
            """,
                    ),
                    ("user", "{user_input}"),
                ]
            )

            return prompt.format_messages(
                user_input=user_message, chat_history=formatted_history
            )

    async def generate_full_response(
        self, message: str, session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """ë¹„ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì „ì²´ ì‘ë‹µì„ í•œ ë²ˆì— ìƒì„±í•©ë‹ˆë‹¤."""

        full_response = ""
        category = "unknown"
        sources = []

        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ëª¨ë‘ ìˆ˜ì§‘
        async for chunk in self.stream_chat_response(message, session_id):
            if chunk["type"] == "final":
                full_response = chunk["content"]
                category = chunk.get("category", "unknown")
                break

        return {"response": full_response, "category": category, "sources": sources}

    def get_conversation_history(self, session_id: str) -> List[Dict[str, Any]]:
        """ëŒ€í™” ížˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
        if session_id not in self.conversation_histories:
            return []

        history = []
        for msg in self.conversation_histories[session_id]:
            history.append(
                {
                    "type": "human" if isinstance(msg, HumanMessage) else "ai",
                    "content": msg.content,
                    "timestamp": datetime.now().isoformat(),
                }
            )

        return history

    def clear_conversation_history(self, session_id: str):
        """ëŒ€í™” ížˆìŠ¤í† ë¦¬ ì‚­ì œ"""
        if session_id in self.conversation_histories:
            del self.conversation_histories[session_id]
