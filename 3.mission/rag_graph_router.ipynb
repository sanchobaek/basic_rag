{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bcab1d9",
   "metadata": {},
   "source": [
    "라이브러리 임포트\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed375125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Optional, Literal, List, Dict, Any\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "import enum\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import uuid\n",
    "\n",
    "# LangChain 및 기타 필요한 라이브러리 임포트\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "\n",
    "# LangGraph 관련 임포트\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# 기존 RAG 코드에서 가져온 컴포넌트\n",
    "from langchain_upstage import UpstageDocumentParseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 랭퓨즈\n",
    "from langfuse import Langfuse, get_client\n",
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# 스트리밍\n",
    "import asyncio\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a8265c",
   "metadata": {},
   "source": [
    "설정 및 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edac3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .env 파일 로드 (필요한 경우)\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4087a03a",
   "metadata": {},
   "source": [
    "유틸리티 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a32285bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format documents function\n",
    "def format_docs(docs):\n",
    "    \"\"\"Format documents into a single string\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Initialize document loading and processing\n",
    "def initialize_rag_components(file_path: str = \"./test_modified.pdf\"):\n",
    "    \"\"\"Initialize all components for RAG\"\"\"\n",
    "    print(\"문서 로딩 중...\")\n",
    "\n",
    "    # Document loading\n",
    "    loader = UpstageDocumentParseLoader(\n",
    "        file_path,\n",
    "        split=\"page\",\n",
    "        output_format=\"markdown\",\n",
    "        ocr=\"auto\",\n",
    "        coordinates=True,\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    print(f\"문서 로딩 완료: {len(docs)} 페이지\")\n",
    "\n",
    "    # Document chunking\n",
    "    print(\"문서 청킹 중...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
    "    docs_splitter = splitter.split_documents(docs)\n",
    "    print(f\"청킹 완료: {len(docs_splitter)} 청크\")\n",
    "\n",
    "    # Embedding model\n",
    "    print(\"임베딩 모델 로딩 중...\")\n",
    "    # Change 'mps' to 'cuda' for NVIDIA GPUs or 'cpu' if you don't have GPU\n",
    "    device = \"cpu\"  # 기본값으로 CPU 사용\n",
    "    try:\n",
    "        import torch\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "        elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "            device = \"mps\"\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "    print(f\"사용 중인 디바이스: {device}\")\n",
    "\n",
    "    hf_embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"intfloat/multilingual-e5-large-instruct\",\n",
    "        model_kwargs={\"device\": device},\n",
    "        encode_kwargs={\"normalize_embeddings\": True},\n",
    "    )\n",
    "\n",
    "    # Vector store\n",
    "    print(\"벡터 스토어 생성 중...\")\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents=docs_splitter,\n",
    "        embedding=hf_embeddings,\n",
    "    )\n",
    "    print(\"벡터 스토어 생성 완료\")\n",
    "\n",
    "    # Retriever\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 5},\n",
    "    )\n",
    "\n",
    "    # LLM\n",
    "    print(\"LLM 초기화 중...\")\n",
    "    llm = ChatAnthropic(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        temperature=0,\n",
    "        streaming=True,  # 🔥 스트리밍 활성화\n",
    "    )\n",
    "    print(\"초기화 완료!\")\n",
    "\n",
    "    return {\n",
    "        \"retriever\": retriever,\n",
    "        \"llm\": llm,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e1f53b",
   "metadata": {},
   "source": [
    "모델 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "77d55733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categories for query classification\n",
    "class Category(enum.Enum):\n",
    "    DOCUMENT = \"document\"  # 문서 관련 질문\n",
    "    GENERAL = \"general\"  # 일반적인 질문\n",
    "    GREETING = \"greeting\"  # 인사말\n",
    "\n",
    "\n",
    "# Pydantic model for structured output\n",
    "class QueryClassification(BaseModel):\n",
    "    \"\"\"사용자 쿼리 분류 모델\"\"\"\n",
    "\n",
    "    category: Category = Field(\n",
    "        description=\"쿼리를 카테고리화 하세요. DOCUMENT(문서 관련 질문) / GENERAL(일반적인 질문) / GREETING(인사) 중에 하나로 구분하세요.\"\n",
    "    )\n",
    "    reasoning: str = Field(description=\"왜 이 카테고리를 선택했는지 설명하세요.\")\n",
    "\n",
    "\n",
    "# Define the state structure\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    context: List[str]\n",
    "    category: Optional[str]\n",
    "\n",
    "\n",
    "# Router function for categorizing queries\n",
    "def router(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"사용자 쿼리를 카테고리로 분류하는 라우터\"\"\"\n",
    "    print(\"쿼리 분류 중...\")\n",
    "\n",
    "    # Get the most recent user message\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "\n",
    "    # Create the router input\n",
    "    router_input = f\"\"\"\n",
    "    다음 사용자 쿼리를 분석하고 카테고리를 결정하세요.\n",
    "    카테고리:\n",
    "    - document: 문서 내용에 관한 질문 (예: \"아주대학교에 대해 알려줘\", \"이 문서에서 중요한 내용은?\")\n",
    "    - general: 일반적인 질문으로, 문서와 관련이 없음 (예: \"오늘 날씨 어때?\", \"파이썬이란?\")\n",
    "    - greeting: 인사말 (예: \"안녕\", \"반가워\", \"뭐해?\")\n",
    "    \n",
    "    쿼리: {user_message}\n",
    "    \"\"\"\n",
    "\n",
    "    # Get LLM\n",
    "    llm = rag_components[\"llm\"]\n",
    "\n",
    "    # Structured output with the classification model\n",
    "    structured_llm = llm.with_structured_output(QueryClassification)\n",
    "\n",
    "    # Get classification with Langfuse tracking\n",
    "    classification = structured_llm.invoke(router_input)\n",
    "\n",
    "    category = classification.category.value\n",
    "    print(f\"분류 결과: {category} (이유: {classification.reasoning})\")\n",
    "\n",
    "    return {\"category\": category}\n",
    "\n",
    "\n",
    "# Conditional routing function\n",
    "def route_by_category(state: State) -> Literal[\"document_qa\", \"general_qa\", \"greeting\"]:\n",
    "    \"\"\"카테고리에 기반하여 다음 노드를 결정\"\"\"\n",
    "    category = state.get(\"category\", \"\").lower()\n",
    "\n",
    "    if category == \"document\":\n",
    "        return \"document_qa\"\n",
    "    elif category == \"general\":\n",
    "        return \"general_qa\"\n",
    "    elif category == \"greeting\":\n",
    "        return \"greeting\"\n",
    "    else:\n",
    "        # 기본값은 일반 질의응답\n",
    "        return \"general_qa\"\n",
    "\n",
    "\n",
    "# Define LangGraph nodes\n",
    "def retrieve_documents(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"문서에서 관련 내용 검색\"\"\"\n",
    "    print(\"문서 검색 중...\")\n",
    "\n",
    "    # Get the most recent user message\n",
    "    user_message = state[\"messages\"][-1]\n",
    "\n",
    "    # Retrieve documents\n",
    "    retriever = rag_components[\"retriever\"]\n",
    "    docs = retriever.invoke(user_message.content)\n",
    "\n",
    "    # Format documents\n",
    "    formatted_docs = format_docs(docs)\n",
    "    print(f\"검색 완료: {len(docs)} 문서 찾음\")\n",
    "\n",
    "    # Return updated state\n",
    "    return {\"context\": [formatted_docs]}\n",
    "\n",
    "\n",
    "def document_qa(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"문서 기반 질의응답\"\"\"\n",
    "    print(\"문서 기반 응답 생성 중...\")\n",
    "    context = state[\"context\"][0] if state[\"context\"] else \"문서 정보 없음\"\n",
    "    # Get user message\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "\n",
    "    # 2. 이전 대화들은 별도로 히스토리 구성\n",
    "    history_messages = state[\"messages\"][:-1]  # [사용자1, AI1, 사용자2, AI2, ...]\n",
    "    formatted_history = \"\"\n",
    "    for msg in history_messages:\n",
    "        role = \"사용자\" if isinstance(msg, HumanMessage) else \"AI\"\n",
    "        formatted_history += f\"{role}: {msg.content}\\n\"\n",
    "\n",
    "    # Create prompt\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"너는 친절한 한국어 AI 비서야. \n",
    "            제공된 문서 내용(context)과 이전 대화 내용을 참고해서 질문에 답해.\n",
    "            반드시 한국어로만 대답하고, 문서에 없는 내용은 대답하지 말고 모른다고 해.\n",
    "            \n",
    "            참고 문서:\n",
    "            {context}\n",
    "            \n",
    "            이전 대화:\n",
    "            {chat_history}\n",
    "            \"\"\",\n",
    "            ),\n",
    "            (\"user\", \"{user_input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Invoke the LLM with Langfuse tracking\n",
    "    llm = rag_components[\"llm\"]\n",
    "\n",
    "    # Create formatted message for LLM\n",
    "    formatted_message = prompt.format_messages(\n",
    "        context=context,  # ← 이 부분이 누락되어 있었음!\n",
    "        chat_history=formatted_history,\n",
    "        user_input=user_message,\n",
    "    )\n",
    "\n",
    "    # Get response from LLM with Langfuse tracking\n",
    "    ai_response = llm.invoke(formatted_message)\n",
    "    response_content = ai_response.content\n",
    "    print(\"문서 기반 응답 생성 완료\")\n",
    "\n",
    "    # Return the assistant message\n",
    "    return {\"messages\": [AIMessage(content=response_content)]}\n",
    "\n",
    "\n",
    "def general_qa(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"일반 질의응답\"\"\"\n",
    "    print(\"일반 응답 생성 중...\")\n",
    "\n",
    "    # Get user message\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "\n",
    "    # 2. 이전 대화들은 별도로 히스토리 구성\n",
    "    history_messages = state[\"messages\"][:-1]  # [사용자1, AI1, 사용자2, AI2, ...]\n",
    "    formatted_history = \"\"\n",
    "    for msg in history_messages:\n",
    "        role = \"사용자\" if isinstance(msg, HumanMessage) else \"AI\"\n",
    "        formatted_history += f\"{role}: {msg.content}\\n\"\n",
    "\n",
    "    # Create prompt\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"너는 친절한 한국어 AI 비서야. \n",
    "            사용자의 질문에 대해 간결하고 정확하게 답변해. 한국어로 대답해.\n",
    "            \n",
    "            이전 대화:\n",
    "            {chat_history}\n",
    "            \"\"\",\n",
    "            ),\n",
    "            (\"user\", \"{user_input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Invoke the LLM with Langfuse tracking\n",
    "    llm = rag_components[\"llm\"]\n",
    "\n",
    "    # Create formatted message for LLM\n",
    "    formatted_message = prompt.format_messages(\n",
    "        user_input=user_message,  # ← 이 부분이 누락되어 있었음!\n",
    "        chat_history=formatted_history,  # ← 이 부분이 누락되어 있었음!\n",
    "    )\n",
    "\n",
    "    # Get response from LLM with Langfuse tracking\n",
    "    ai_response = llm.invoke(formatted_message)\n",
    "    response_content = ai_response.content\n",
    "\n",
    "    print(\"일반 응답 생성 완료\")\n",
    "\n",
    "    # Return the assistant message\n",
    "    return {\"messages\": [AIMessage(content=response_content)]}\n",
    "\n",
    "\n",
    "def greeting(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"인사말에 응답\"\"\"\n",
    "    print(\"인사 응답 생성 중...\")\n",
    "\n",
    "    # Get user message\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "\n",
    "    # 2. 이전 대화들은 별도로 히스토리 구성\n",
    "    history_messages = state[\"messages\"][:-1]  # [사용자1, AI1, 사용자2, AI2, ...]\n",
    "    formatted_history = \"\"\n",
    "    for msg in history_messages:\n",
    "        role = \"사용자\" if isinstance(msg, HumanMessage) else \"AI\"\n",
    "        formatted_history += f\"{role}: {msg.content}\\n\"\n",
    "\n",
    "    # Create prompt\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"너는 친절한 한국어 AI 비서야.\n",
    "            사용자의 인사에 친근하고 따뜻하게 응답해. 간결하게 한국어로 대답해.\n",
    "\n",
    "            이전 대화:\n",
    "            {chat_history}\n",
    "            \"\"\",\n",
    "            ),\n",
    "            (\"user\", \"{user_input}\"),  # ✅ 수정된 부분\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Invoke the LLM with Langfuse tracking\n",
    "    llm = rag_components[\"llm\"]  # 변수 변경하기\n",
    "    # Create formatted message for LLM\n",
    "    formatted_message = prompt.format_messages(\n",
    "        user_input=user_message,\n",
    "        chat_history=formatted_history,  # ← 이 부분이 누락되어 있었음!\n",
    "    )\n",
    "\n",
    "    # Get response from LLM with Langfuse tracking\n",
    "    ai_response = llm.invoke(formatted_message)\n",
    "    response_content = ai_response.content\n",
    "\n",
    "    print(\"인사 응답 생성 완료\")\n",
    "\n",
    "    # Return the assistant message\n",
    "    return {\"messages\": [AIMessage(content=response_content)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7f07b3",
   "metadata": {},
   "source": [
    "그래프 노드 함수 및 그래프 빌드 및 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "144be721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG 컴포넌트 초기화 중...\n",
      "문서 로딩 중...\n",
      "초기화 중 오류 발생: HTTP error: {\"error\":{\"message\":\"API key suspended due to insufficient credit. Register your payment method at https://console.upstage.ai/billing to continue.\",\"type\":\"invalid_request_error\",\"param\":\"\",\"code\":\"api_key_is_not_allowed\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/baegjonghun/.pyenv/versions/3.11.10/lib/python3.11/site-packages/langchain_upstage/document_parse_parsers.py\", line 181, in _get_response\n",
      "    response.raise_for_status()\n",
      "  File \"/Users/baegjonghun/.pyenv/versions/3.11.10/lib/python3.11/site-packages/requests/models.py\", line 1026, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://api.upstage.ai/v1/document-ai/document-parse\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/4n/tjmm0jpn36jdkkfvpw1g79zw0000gn/T/ipykernel_77504/2322629006.py\", line 97, in <module>\n",
      "    rag_components = initialize_rag_components()\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/4n/tjmm0jpn36jdkkfvpw1g79zw0000gn/T/ipykernel_77504/3891573948.py\", line 20, in initialize_rag_components\n",
      "    docs = loader.load()\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/baegjonghun/.pyenv/versions/3.11.10/lib/python3.11/site-packages/langchain_upstage/document_parse.py\", line 165, in load\n",
      "    return list(self.parser.lazy_parse(blob, is_batch=True))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/baegjonghun/.pyenv/versions/3.11.10/lib/python3.11/site-packages/langchain_upstage/document_parse_parsers.py\", line 434, in lazy_parse\n",
      "    elements = self._split_and_request(full_docs, start_page, num_pages)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/baegjonghun/.pyenv/versions/3.11.10/lib/python3.11/site-packages/langchain_upstage/document_parse_parsers.py\", line 225, in _split_and_request\n",
      "    response = self._get_response({\"document\": buffer})\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/baegjonghun/.pyenv/versions/3.11.10/lib/python3.11/site-packages/langchain_upstage/document_parse_parsers.py\", line 185, in _get_response\n",
      "    raise ValueError(f\"HTTP error: {e.response.text}\")\n",
      "ValueError: HTTP error: {\"error\":{\"message\":\"API key suspended due to insufficient credit. Register your payment method at https://console.upstage.ai/billing to continue.\",\"type\":\"invalid_request_error\",\"param\":\"\",\"code\":\"api_key_is_not_allowed\"}}\n"
     ]
    }
   ],
   "source": [
    "conversation_history = []  # 전체 대화 히스토리 저장\n",
    "\n",
    "\n",
    "# Run the graph with a given user input\n",
    "def run_graph(user_input: str):\n",
    "    global conversation_history\n",
    "\n",
    "    # 새로운 사용자 메시지를 히스토리에 추가\n",
    "    user_message = HumanMessage(content=user_input)\n",
    "    conversation_history.append(user_message)\n",
    "\n",
    "    \"\"\"Run the graph with a user input and return the response\"\"\"\n",
    "    # Create initial state with the user message\n",
    "    initial_state = {\n",
    "        \"messages\": conversation_history,  # 전체 대화 히스토리가 누적된 배열\n",
    "        \"context\": [],\n",
    "        \"category\": None,\n",
    "    }\n",
    "\n",
    "    # Run the graph and get the final state\n",
    "    result = graph.invoke(initial_state)\n",
    "\n",
    "    # Extract the AI response\n",
    "    if \"messages\" in result and len(result[\"messages\"]) > 1:\n",
    "        # Get the assistant message (should be the last one)\n",
    "        ai_msg = result[\"messages\"][-1]\n",
    "        if isinstance(ai_msg, AIMessage):\n",
    "            # ✅ AI 응답을 conversation_history에 추가 (핵심 수정사항!)\n",
    "            conversation_history.append(ai_msg)\n",
    "            return ai_msg.content\n",
    "\n",
    "    return \"응답을 생성할 수 없습니다.\"\n",
    "\n",
    "\n",
    "# Interactive chat interface\n",
    "def interactive_chat():\n",
    "    \"\"\"Interactive chat interface for the RAG system\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"🤖 LangGraph 라우팅 RAG 챗봇 시작 (멀티턴 대화 지원)\")\n",
    "    print(\"💡 '종료' 입력 시 대화를 끝냅니다.\")\n",
    "    print(\"💡 '히스토리' 입력 시 현재 대화 히스토리를 확인합니다.\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"\\n🙋 사용자: \").strip()\n",
    "\n",
    "                if not user_input:\n",
    "                    print(\"❗ 메시지를 입력해주세요.\")\n",
    "                    continue\n",
    "\n",
    "                if user_input.lower() == \"종료\":\n",
    "                    print(\"👋 채팅을 종료합니다!\")\n",
    "                    break\n",
    "\n",
    "                # 히스토리 확인 명령어 추가\n",
    "                if user_input.lower() == \"히스토리\":\n",
    "                    print(\"\\n=== 현재 대화 히스토리 ===\")\n",
    "                    for i, msg in enumerate(conversation_history):\n",
    "                        msg_type = \"사용자\" if isinstance(msg, HumanMessage) else \"AI\"\n",
    "                        content = (\n",
    "                            msg.content[:100] + \"...\"\n",
    "                            if len(msg.content) > 100\n",
    "                            else msg.content\n",
    "                        )\n",
    "                        print(f\"{i+1}. {msg_type}: {content}\")\n",
    "                    print(f\"총 {len(conversation_history)}개 메시지\")\n",
    "                    print(\"=\" * 30)\n",
    "                    continue\n",
    "\n",
    "                # Get response\n",
    "                print(\"🤖 AI: \", end=\"\", flush=True)\n",
    "                response = run_graph(user_input)\n",
    "                print(response)\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n👋 채팅을 종료합니다!\")\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {str(e)}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "# Global variable for RAG components\n",
    "rag_components = None\n",
    "graph = None\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Initialize RAG components\n",
    "        print(\"RAG 컴포넌트 초기화 중...\")\n",
    "        rag_components = initialize_rag_components()\n",
    "\n",
    "        # Build the LangGraph\n",
    "        print(\"LangGraph 구성 중...\")\n",
    "        graph_builder = StateGraph(State)\n",
    "\n",
    "        # Add nodes\n",
    "        graph_builder.add_node(\"router\", router)\n",
    "        graph_builder.add_node(\"retrieve\", retrieve_documents)\n",
    "        graph_builder.add_node(\"document_qa\", document_qa)\n",
    "        graph_builder.add_node(\"general_qa\", general_qa)\n",
    "        graph_builder.add_node(\"greeting\", greeting)\n",
    "\n",
    "        # Add edges\n",
    "        graph_builder.add_edge(START, \"router\")\n",
    "\n",
    "        # Add conditional edges based on the category\n",
    "        graph_builder.add_conditional_edges(\n",
    "            \"router\",\n",
    "            route_by_category,\n",
    "            {\n",
    "                \"document_qa\": \"retrieve\",\n",
    "                \"general_qa\": \"general_qa\",\n",
    "                \"greeting\": \"greeting\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # Connect retrieve to document_qa\n",
    "        graph_builder.add_edge(\"retrieve\", \"document_qa\")\n",
    "\n",
    "        # Connect all output nodes to END\n",
    "        graph_builder.add_edge(\"document_qa\", END)\n",
    "        graph_builder.add_edge(\"general_qa\", END)\n",
    "        graph_builder.add_edge(\"greeting\", END)\n",
    "\n",
    "        # Compile the graph\n",
    "        graph = graph_builder.compile().with_config(\n",
    "            config={\"callbacks\": [langfuse_handler]}\n",
    "        )\n",
    "        print(\"LangGraph 구성 완료!\")\n",
    "\n",
    "        # Try to visualize the graph if possible\n",
    "        try:\n",
    "            print(\"그래프 시각화 시도 중...\")\n",
    "            from IPython.display import Image, display\n",
    "\n",
    "            display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "            print(\"그래프 시각화 완료!\")\n",
    "        except Exception as e:\n",
    "            print(f\"그래프 시각화 실패: {e}\")\n",
    "\n",
    "        # Start interactive chat\n",
    "        interactive_chat()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"초기화 중 오류 발생: {str(e)}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779f005d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
