{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bcab1d9",
   "metadata": {},
   "source": [
    "ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed375125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Optional, Literal, List, Dict, Any\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "import enum\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import uuid\n",
    "\n",
    "# LangChain ë° ê¸°íƒ€ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "\n",
    "# LangGraph ê´€ë ¨ ì„í¬íŠ¸\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# ê¸°ì¡´ RAG ì½”ë“œì—ì„œ ê°€ì ¸ì˜¨ ì»´í¬ë„ŒíŠ¸\n",
    "from langchain_upstage import UpstageDocumentParseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ë­í“¨ì¦ˆ\n",
    "from langfuse import Langfuse, get_client\n",
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# ìŠ¤íŠ¸ë¦¬ë°\n",
    "import asyncio\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a8265c",
   "metadata": {},
   "source": [
    "ì„¤ì • ë° ì´ˆê¸°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edac3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .env íŒŒì¼ ë¡œë“œ (í•„ìš”í•œ ê²½ìš°)\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4087a03a",
   "metadata": {},
   "source": [
    "ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a32285bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format documents function\n",
    "def format_docs(docs):\n",
    "    \"\"\"Format documents into a single string\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Initialize document loading and processing\n",
    "def initialize_rag_components(file_path: str = \"./test_modified.pdf\"):\n",
    "    \"\"\"Initialize all components for RAG\"\"\"\n",
    "    print(\"ë¬¸ì„œ ë¡œë”© ì¤‘...\")\n",
    "\n",
    "    # Document loading\n",
    "    loader = UpstageDocumentParseLoader(\n",
    "        file_path,\n",
    "        split=\"page\",\n",
    "        output_format=\"markdown\",\n",
    "        ocr=\"auto\",\n",
    "        coordinates=True,\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    print(f\"ë¬¸ì„œ ë¡œë”© ì™„ë£Œ: {len(docs)} í˜ì´ì§€\")\n",
    "\n",
    "    # Document chunking\n",
    "    print(\"ë¬¸ì„œ ì²­í‚¹ ì¤‘...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
    "    docs_splitter = splitter.split_documents(docs)\n",
    "    print(f\"ì²­í‚¹ ì™„ë£Œ: {len(docs_splitter)} ì²­í¬\")\n",
    "\n",
    "    # Embedding model\n",
    "    print(\"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "    # Change 'mps' to 'cuda' for NVIDIA GPUs or 'cpu' if you don't have GPU\n",
    "    device = \"cpu\"  # ê¸°ë³¸ê°’ìœ¼ë¡œ CPU ì‚¬ìš©\n",
    "    try:\n",
    "        import torch\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "        elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "            device = \"mps\"\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "    print(f\"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "    hf_embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"intfloat/multilingual-e5-large-instruct\",\n",
    "        model_kwargs={\"device\": device},\n",
    "        encode_kwargs={\"normalize_embeddings\": True},\n",
    "    )\n",
    "\n",
    "    # Vector store\n",
    "    print(\"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘...\")\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents=docs_splitter,\n",
    "        embedding=hf_embeddings,\n",
    "    )\n",
    "    print(\"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "    # Retriever\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 5},\n",
    "    )\n",
    "\n",
    "    # LLM\n",
    "    print(\"LLM ì´ˆê¸°í™” ì¤‘...\")\n",
    "    llm = ChatAnthropic(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        temperature=0,\n",
    "        streaming=True,  # ğŸ”¥ ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”\n",
    "    )\n",
    "    print(\"ì´ˆê¸°í™” ì™„ë£Œ!\")\n",
    "\n",
    "    return {\n",
    "        \"retriever\": retriever,\n",
    "        \"llm\": llm,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e1f53b",
   "metadata": {},
   "source": [
    "ëª¨ë¸ í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "77d55733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categories for query classification\n",
    "class Category(enum.Enum):\n",
    "    DOCUMENT = \"document\"  # ë¬¸ì„œ ê´€ë ¨ ì§ˆë¬¸\n",
    "    GENERAL = \"general\"  # ì¼ë°˜ì ì¸ ì§ˆë¬¸\n",
    "    GREETING = \"greeting\"  # ì¸ì‚¬ë§\n",
    "\n",
    "\n",
    "# Pydantic model for structured output\n",
    "class QueryClassification(BaseModel):\n",
    "    \"\"\"ì‚¬ìš©ì ì¿¼ë¦¬ ë¶„ë¥˜ ëª¨ë¸\"\"\"\n",
    "\n",
    "    category: Category = Field(\n",
    "        description=\"ì¿¼ë¦¬ë¥¼ ì¹´í…Œê³ ë¦¬í™” í•˜ì„¸ìš”. DOCUMENT(ë¬¸ì„œ ê´€ë ¨ ì§ˆë¬¸) / GENERAL(ì¼ë°˜ì ì¸ ì§ˆë¬¸) / GREETING(ì¸ì‚¬) ì¤‘ì— í•˜ë‚˜ë¡œ êµ¬ë¶„í•˜ì„¸ìš”.\"\n",
    "    )\n",
    "    reasoning: str = Field(description=\"ì™œ ì´ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí–ˆëŠ”ì§€ ì„¤ëª…í•˜ì„¸ìš”.\")\n",
    "\n",
    "\n",
    "# Define the state structure\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    context: List[str]\n",
    "    category: Optional[str]\n",
    "\n",
    "\n",
    "# Router function for categorizing queries\n",
    "def router(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ëŠ” ë¼ìš°í„°\"\"\"\n",
    "    print(\"ì¿¼ë¦¬ ë¶„ë¥˜ ì¤‘...\")\n",
    "\n",
    "    # Get the most recent user message\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "\n",
    "    # Create the router input\n",
    "    router_input = f\"\"\"\n",
    "    ë‹¤ìŒ ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ê³  ì¹´í…Œê³ ë¦¬ë¥¼ ê²°ì •í•˜ì„¸ìš”.\n",
    "    ì¹´í…Œê³ ë¦¬:\n",
    "    - document: ë¬¸ì„œ ë‚´ìš©ì— ê´€í•œ ì§ˆë¬¸ (ì˜ˆ: \"ì•„ì£¼ëŒ€í•™êµì— ëŒ€í•´ ì•Œë ¤ì¤˜\", \"ì´ ë¬¸ì„œì—ì„œ ì¤‘ìš”í•œ ë‚´ìš©ì€?\")\n",
    "    - general: ì¼ë°˜ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ, ë¬¸ì„œì™€ ê´€ë ¨ì´ ì—†ìŒ (ì˜ˆ: \"ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?\", \"íŒŒì´ì¬ì´ë€?\")\n",
    "    - greeting: ì¸ì‚¬ë§ (ì˜ˆ: \"ì•ˆë…•\", \"ë°˜ê°€ì›Œ\", \"ë­í•´?\")\n",
    "    \n",
    "    ì¿¼ë¦¬: {user_message}\n",
    "    \"\"\"\n",
    "\n",
    "    # Get LLM\n",
    "    llm = rag_components[\"llm\"]\n",
    "\n",
    "    # Structured output with the classification model\n",
    "    structured_llm = llm.with_structured_output(QueryClassification)\n",
    "\n",
    "    # Get classification with Langfuse tracking\n",
    "    classification = structured_llm.invoke(router_input)\n",
    "\n",
    "    category = classification.category.value\n",
    "    print(f\"ë¶„ë¥˜ ê²°ê³¼: {category} (ì´ìœ : {classification.reasoning})\")\n",
    "\n",
    "    return {\"category\": category}\n",
    "\n",
    "\n",
    "# Conditional routing function\n",
    "def route_by_category(state: State) -> Literal[\"document_qa\", \"general_qa\", \"greeting\"]:\n",
    "    \"\"\"ì¹´í…Œê³ ë¦¬ì— ê¸°ë°˜í•˜ì—¬ ë‹¤ìŒ ë…¸ë“œë¥¼ ê²°ì •\"\"\"\n",
    "    category = state.get(\"category\", \"\").lower()\n",
    "\n",
    "    if category == \"document\":\n",
    "        return \"document_qa\"\n",
    "    elif category == \"general\":\n",
    "        return \"general_qa\"\n",
    "    elif category == \"greeting\":\n",
    "        return \"greeting\"\n",
    "    else:\n",
    "        # ê¸°ë³¸ê°’ì€ ì¼ë°˜ ì§ˆì˜ì‘ë‹µ\n",
    "        return \"general_qa\"\n",
    "\n",
    "\n",
    "# Define LangGraph nodes\n",
    "def retrieve_documents(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš© ê²€ìƒ‰\"\"\"\n",
    "    print(\"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...\")\n",
    "\n",
    "    # Get the most recent user message\n",
    "    user_message = state[\"messages\"][-1]\n",
    "\n",
    "    # Retrieve documents\n",
    "    retriever = rag_components[\"retriever\"]\n",
    "    docs = retriever.invoke(user_message.content)\n",
    "\n",
    "    # Format documents\n",
    "    formatted_docs = format_docs(docs)\n",
    "    print(f\"ê²€ìƒ‰ ì™„ë£Œ: {len(docs)} ë¬¸ì„œ ì°¾ìŒ\")\n",
    "\n",
    "    # Return updated state\n",
    "    return {\"context\": [formatted_docs]}\n",
    "\n",
    "\n",
    "def document_qa(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ\"\"\"\n",
    "    print(\"ë¬¸ì„œ ê¸°ë°˜ ì‘ë‹µ ìƒì„± ì¤‘...\")\n",
    "    context = state[\"context\"][0] if state[\"context\"] else \"ë¬¸ì„œ ì •ë³´ ì—†ìŒ\"\n",
    "    # Get user message\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "\n",
    "    # 2. ì´ì „ ëŒ€í™”ë“¤ì€ ë³„ë„ë¡œ íˆìŠ¤í† ë¦¬ êµ¬ì„±\n",
    "    history_messages = state[\"messages\"][:-1]  # [ì‚¬ìš©ì1, AI1, ì‚¬ìš©ì2, AI2, ...]\n",
    "    formatted_history = \"\"\n",
    "    for msg in history_messages:\n",
    "        role = \"ì‚¬ìš©ì\" if isinstance(msg, HumanMessage) else \"AI\"\n",
    "        formatted_history += f\"{role}: {msg.content}\\n\"\n",
    "\n",
    "    # Create prompt\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"ë„ˆëŠ” ì¹œì ˆí•œ í•œêµ­ì–´ AI ë¹„ì„œì•¼. \n",
    "            ì œê³µëœ ë¬¸ì„œ ë‚´ìš©(context)ê³¼ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µí•´.\n",
    "            ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ëŒ€ë‹µí•˜ê³ , ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ëŒ€ë‹µí•˜ì§€ ë§ê³  ëª¨ë¥¸ë‹¤ê³  í•´.\n",
    "            \n",
    "            ì°¸ê³  ë¬¸ì„œ:\n",
    "            {context}\n",
    "            \n",
    "            ì´ì „ ëŒ€í™”:\n",
    "            {chat_history}\n",
    "            \"\"\",\n",
    "            ),\n",
    "            (\"user\", \"{user_input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Invoke the LLM with Langfuse tracking\n",
    "    llm = rag_components[\"llm\"]\n",
    "\n",
    "    # Create formatted message for LLM\n",
    "    formatted_message = prompt.format_messages(\n",
    "        context=context,  # â† ì´ ë¶€ë¶„ì´ ëˆ„ë½ë˜ì–´ ìˆì—ˆìŒ!\n",
    "        chat_history=formatted_history,\n",
    "        user_input=user_message,\n",
    "    )\n",
    "\n",
    "    # Get response from LLM with Langfuse tracking\n",
    "    ai_response = llm.invoke(formatted_message)\n",
    "    response_content = ai_response.content\n",
    "    print(\"ë¬¸ì„œ ê¸°ë°˜ ì‘ë‹µ ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "    # Return the assistant message\n",
    "    return {\"messages\": [AIMessage(content=response_content)]}\n",
    "\n",
    "\n",
    "def general_qa(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"ì¼ë°˜ ì§ˆì˜ì‘ë‹µ\"\"\"\n",
    "    print(\"ì¼ë°˜ ì‘ë‹µ ìƒì„± ì¤‘...\")\n",
    "\n",
    "    # Get user message\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "\n",
    "    # 2. ì´ì „ ëŒ€í™”ë“¤ì€ ë³„ë„ë¡œ íˆìŠ¤í† ë¦¬ êµ¬ì„±\n",
    "    history_messages = state[\"messages\"][:-1]  # [ì‚¬ìš©ì1, AI1, ì‚¬ìš©ì2, AI2, ...]\n",
    "    formatted_history = \"\"\n",
    "    for msg in history_messages:\n",
    "        role = \"ì‚¬ìš©ì\" if isinstance(msg, HumanMessage) else \"AI\"\n",
    "        formatted_history += f\"{role}: {msg.content}\\n\"\n",
    "\n",
    "    # Create prompt\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"ë„ˆëŠ” ì¹œì ˆí•œ í•œêµ­ì–´ AI ë¹„ì„œì•¼. \n",
    "            ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´. í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•´.\n",
    "            \n",
    "            ì´ì „ ëŒ€í™”:\n",
    "            {chat_history}\n",
    "            \"\"\",\n",
    "            ),\n",
    "            (\"user\", \"{user_input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Invoke the LLM with Langfuse tracking\n",
    "    llm = rag_components[\"llm\"]\n",
    "\n",
    "    # Create formatted message for LLM\n",
    "    formatted_message = prompt.format_messages(\n",
    "        user_input=user_message,  # â† ì´ ë¶€ë¶„ì´ ëˆ„ë½ë˜ì–´ ìˆì—ˆìŒ!\n",
    "        chat_history=formatted_history,  # â† ì´ ë¶€ë¶„ì´ ëˆ„ë½ë˜ì–´ ìˆì—ˆìŒ!\n",
    "    )\n",
    "\n",
    "    # Get response from LLM with Langfuse tracking\n",
    "    ai_response = llm.invoke(formatted_message)\n",
    "    response_content = ai_response.content\n",
    "\n",
    "    print(\"ì¼ë°˜ ì‘ë‹µ ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "    # Return the assistant message\n",
    "    return {\"messages\": [AIMessage(content=response_content)]}\n",
    "\n",
    "\n",
    "def greeting(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"ì¸ì‚¬ë§ì— ì‘ë‹µ\"\"\"\n",
    "    print(\"ì¸ì‚¬ ì‘ë‹µ ìƒì„± ì¤‘...\")\n",
    "\n",
    "    # Get user message\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "\n",
    "    # 2. ì´ì „ ëŒ€í™”ë“¤ì€ ë³„ë„ë¡œ íˆìŠ¤í† ë¦¬ êµ¬ì„±\n",
    "    history_messages = state[\"messages\"][:-1]  # [ì‚¬ìš©ì1, AI1, ì‚¬ìš©ì2, AI2, ...]\n",
    "    formatted_history = \"\"\n",
    "    for msg in history_messages:\n",
    "        role = \"ì‚¬ìš©ì\" if isinstance(msg, HumanMessage) else \"AI\"\n",
    "        formatted_history += f\"{role}: {msg.content}\\n\"\n",
    "\n",
    "    # Create prompt\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"ë„ˆëŠ” ì¹œì ˆí•œ í•œêµ­ì–´ AI ë¹„ì„œì•¼.\n",
    "            ì‚¬ìš©ìì˜ ì¸ì‚¬ì— ì¹œê·¼í•˜ê³  ë”°ëœ»í•˜ê²Œ ì‘ë‹µí•´. ê°„ê²°í•˜ê²Œ í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•´.\n",
    "\n",
    "            ì´ì „ ëŒ€í™”:\n",
    "            {chat_history}\n",
    "            \"\"\",\n",
    "            ),\n",
    "            (\"user\", \"{user_input}\"),  # âœ… ìˆ˜ì •ëœ ë¶€ë¶„\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Invoke the LLM with Langfuse tracking\n",
    "    llm = rag_components[\"llm\"]  # ë³€ìˆ˜ ë³€ê²½í•˜ê¸°\n",
    "    # Create formatted message for LLM\n",
    "    formatted_message = prompt.format_messages(\n",
    "        user_input=user_message,\n",
    "        chat_history=formatted_history,  # â† ì´ ë¶€ë¶„ì´ ëˆ„ë½ë˜ì–´ ìˆì—ˆìŒ!\n",
    "    )\n",
    "\n",
    "    # Get response from LLM with Langfuse tracking\n",
    "    ai_response = llm.invoke(formatted_message)\n",
    "    response_content = ai_response.content\n",
    "\n",
    "    print(\"ì¸ì‚¬ ì‘ë‹µ ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "    # Return the assistant message\n",
    "    return {\"messages\": [AIMessage(content=response_content)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7f07b3",
   "metadata": {},
   "source": [
    "ê·¸ë˜í”„ ë…¸ë“œ í•¨ìˆ˜ ë° ê·¸ë˜í”„ ë¹Œë“œ ë° ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "144be721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì¤‘...\n",
      "ë¬¸ì„œ ë¡œë”© ì¤‘...\n",
      "ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: HTTP error: {\"error\":{\"message\":\"API key suspended due to insufficient credit. Register your payment method at https://console.upstage.ai/billing to continue.\",\"type\":\"invalid_request_error\",\"param\":\"\",\"code\":\"api_key_is_not_allowed\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/baegjonghun/.pyenv/versions/3.11.10/lib/python3.11/site-packages/langchain_upstage/document_parse_parsers.py\", line 181, in _get_response\n",
      "    response.raise_for_status()\n",
      "  File \"/Users/baegjonghun/.pyenv/versions/3.11.10/lib/python3.11/site-packages/requests/models.py\", line 1026, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://api.upstage.ai/v1/document-ai/document-parse\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/4n/tjmm0jpn36jdkkfvpw1g79zw0000gn/T/ipykernel_77504/2322629006.py\", line 97, in <module>\n",
      "    rag_components = initialize_rag_components()\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/4n/tjmm0jpn36jdkkfvpw1g79zw0000gn/T/ipykernel_77504/3891573948.py\", line 20, in initialize_rag_components\n",
      "    docs = loader.load()\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/baegjonghun/.pyenv/versions/3.11.10/lib/python3.11/site-packages/langchain_upstage/document_parse.py\", line 165, in load\n",
      "    return list(self.parser.lazy_parse(blob, is_batch=True))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/baegjonghun/.pyenv/versions/3.11.10/lib/python3.11/site-packages/langchain_upstage/document_parse_parsers.py\", line 434, in lazy_parse\n",
      "    elements = self._split_and_request(full_docs, start_page, num_pages)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/baegjonghun/.pyenv/versions/3.11.10/lib/python3.11/site-packages/langchain_upstage/document_parse_parsers.py\", line 225, in _split_and_request\n",
      "    response = self._get_response({\"document\": buffer})\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/baegjonghun/.pyenv/versions/3.11.10/lib/python3.11/site-packages/langchain_upstage/document_parse_parsers.py\", line 185, in _get_response\n",
      "    raise ValueError(f\"HTTP error: {e.response.text}\")\n",
      "ValueError: HTTP error: {\"error\":{\"message\":\"API key suspended due to insufficient credit. Register your payment method at https://console.upstage.ai/billing to continue.\",\"type\":\"invalid_request_error\",\"param\":\"\",\"code\":\"api_key_is_not_allowed\"}}\n"
     ]
    }
   ],
   "source": [
    "conversation_history = []  # ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥\n",
    "\n",
    "\n",
    "# Run the graph with a given user input\n",
    "def run_graph(user_input: str):\n",
    "    global conversation_history\n",
    "\n",
    "    # ìƒˆë¡œìš´ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€\n",
    "    user_message = HumanMessage(content=user_input)\n",
    "    conversation_history.append(user_message)\n",
    "\n",
    "    \"\"\"Run the graph with a user input and return the response\"\"\"\n",
    "    # Create initial state with the user message\n",
    "    initial_state = {\n",
    "        \"messages\": conversation_history,  # ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ëˆ„ì ëœ ë°°ì—´\n",
    "        \"context\": [],\n",
    "        \"category\": None,\n",
    "    }\n",
    "\n",
    "    # Run the graph and get the final state\n",
    "    result = graph.invoke(initial_state)\n",
    "\n",
    "    # Extract the AI response\n",
    "    if \"messages\" in result and len(result[\"messages\"]) > 1:\n",
    "        # Get the assistant message (should be the last one)\n",
    "        ai_msg = result[\"messages\"][-1]\n",
    "        if isinstance(ai_msg, AIMessage):\n",
    "            # âœ… AI ì‘ë‹µì„ conversation_historyì— ì¶”ê°€ (í•µì‹¬ ìˆ˜ì •ì‚¬í•­!)\n",
    "            conversation_history.append(ai_msg)\n",
    "            return ai_msg.content\n",
    "\n",
    "    return \"ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "\n",
    "# Interactive chat interface\n",
    "def interactive_chat():\n",
    "    \"\"\"Interactive chat interface for the RAG system\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ¤– LangGraph ë¼ìš°íŒ… RAG ì±—ë´‡ ì‹œì‘ (ë©€í‹°í„´ ëŒ€í™” ì§€ì›)\")\n",
    "    print(\"ğŸ’¡ 'ì¢…ë£Œ' ì…ë ¥ ì‹œ ëŒ€í™”ë¥¼ ëëƒ…ë‹ˆë‹¤.\")\n",
    "    print(\"ğŸ’¡ 'íˆìŠ¤í† ë¦¬' ì…ë ¥ ì‹œ í˜„ì¬ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"\\nğŸ™‹ ì‚¬ìš©ì: \").strip()\n",
    "\n",
    "                if not user_input:\n",
    "                    print(\"â— ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\")\n",
    "                    continue\n",
    "\n",
    "                if user_input.lower() == \"ì¢…ë£Œ\":\n",
    "                    print(\"ğŸ‘‹ ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤!\")\n",
    "                    break\n",
    "\n",
    "                # íˆìŠ¤í† ë¦¬ í™•ì¸ ëª…ë ¹ì–´ ì¶”ê°€\n",
    "                if user_input.lower() == \"íˆìŠ¤í† ë¦¬\":\n",
    "                    print(\"\\n=== í˜„ì¬ ëŒ€í™” íˆìŠ¤í† ë¦¬ ===\")\n",
    "                    for i, msg in enumerate(conversation_history):\n",
    "                        msg_type = \"ì‚¬ìš©ì\" if isinstance(msg, HumanMessage) else \"AI\"\n",
    "                        content = (\n",
    "                            msg.content[:100] + \"...\"\n",
    "                            if len(msg.content) > 100\n",
    "                            else msg.content\n",
    "                        )\n",
    "                        print(f\"{i+1}. {msg_type}: {content}\")\n",
    "                    print(f\"ì´ {len(conversation_history)}ê°œ ë©”ì‹œì§€\")\n",
    "                    print(\"=\" * 30)\n",
    "                    continue\n",
    "\n",
    "                # Get response\n",
    "                print(\"ğŸ¤– AI: \", end=\"\", flush=True)\n",
    "                response = run_graph(user_input)\n",
    "                print(response)\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nğŸ‘‹ ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤!\")\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "# Global variable for RAG components\n",
    "rag_components = None\n",
    "graph = None\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Initialize RAG components\n",
    "        print(\"RAG ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì¤‘...\")\n",
    "        rag_components = initialize_rag_components()\n",
    "\n",
    "        # Build the LangGraph\n",
    "        print(\"LangGraph êµ¬ì„± ì¤‘...\")\n",
    "        graph_builder = StateGraph(State)\n",
    "\n",
    "        # Add nodes\n",
    "        graph_builder.add_node(\"router\", router)\n",
    "        graph_builder.add_node(\"retrieve\", retrieve_documents)\n",
    "        graph_builder.add_node(\"document_qa\", document_qa)\n",
    "        graph_builder.add_node(\"general_qa\", general_qa)\n",
    "        graph_builder.add_node(\"greeting\", greeting)\n",
    "\n",
    "        # Add edges\n",
    "        graph_builder.add_edge(START, \"router\")\n",
    "\n",
    "        # Add conditional edges based on the category\n",
    "        graph_builder.add_conditional_edges(\n",
    "            \"router\",\n",
    "            route_by_category,\n",
    "            {\n",
    "                \"document_qa\": \"retrieve\",\n",
    "                \"general_qa\": \"general_qa\",\n",
    "                \"greeting\": \"greeting\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # Connect retrieve to document_qa\n",
    "        graph_builder.add_edge(\"retrieve\", \"document_qa\")\n",
    "\n",
    "        # Connect all output nodes to END\n",
    "        graph_builder.add_edge(\"document_qa\", END)\n",
    "        graph_builder.add_edge(\"general_qa\", END)\n",
    "        graph_builder.add_edge(\"greeting\", END)\n",
    "\n",
    "        # Compile the graph\n",
    "        graph = graph_builder.compile().with_config(\n",
    "            config={\"callbacks\": [langfuse_handler]}\n",
    "        )\n",
    "        print(\"LangGraph êµ¬ì„± ì™„ë£Œ!\")\n",
    "\n",
    "        # Try to visualize the graph if possible\n",
    "        try:\n",
    "            print(\"ê·¸ë˜í”„ ì‹œê°í™” ì‹œë„ ì¤‘...\")\n",
    "            from IPython.display import Image, display\n",
    "\n",
    "            display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "            print(\"ê·¸ë˜í”„ ì‹œê°í™” ì™„ë£Œ!\")\n",
    "        except Exception as e:\n",
    "            print(f\"ê·¸ë˜í”„ ì‹œê°í™” ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "        # Start interactive chat\n",
    "        interactive_chat()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779f005d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
