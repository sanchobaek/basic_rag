{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bcab1d9",
   "metadata": {},
   "source": [
    "라이브러리 임포트\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed375125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Optional, Literal, List, Dict, Any\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "import enum\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import uuid\n",
    "\n",
    "# LangChain 및 기타 필요한 라이브러리 임포트\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "\n",
    "# LangGraph 관련 임포트\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# 기존 RAG 코드에서 가져온 컴포넌트\n",
    "from langchain_upstage import UpstageDocumentParseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 랭퓨즈\n",
    "from langfuse import Langfuse, get_client\n",
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# 스트리밍\n",
    "import asyncio\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a8265c",
   "metadata": {},
   "source": [
    "설정 및 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edac3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .env 파일 로드 (필요한 경우)\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4087a03a",
   "metadata": {},
   "source": [
    "유틸리티 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32285bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG 컴포넌트 초기화 중...\n",
      "문서 로딩 중...\n",
      "문서 로딩 완료: 40 페이지\n",
      "문서 청킹 중...\n",
      "청킹 완료: 70 청크\n",
      "임베딩 모델 로딩 중...\n",
      "사용 중인 디바이스: mps\n",
      "벡터 스토어 생성 중...\n",
      "벡터 스토어 생성 완료\n",
      "LLM 초기화 중...\n",
      "초기화 완료!\n",
      "🚀 스트리밍 RAG 챗봇 시작!\n",
      "============================================================\n",
      "🤖 LangGraph 라우팅 RAG 챗봇 시작 (스트리밍 지원)\n",
      "💡 '종료' 입력 시 대화를 끝냅니다.\n",
      "💡 '히스토리' 입력 시 현재 대화 히스토리를 확인합니다.\n",
      "============================================================\n",
      "쿼리 분류 중...\n",
      "분류 결과: document (이유: 이 쿼리는 아주대학교 공과대학에 대한 정보를 요청하고 있으므로 document 카테고리에 해당합니다.)\n",
      "문서 검색 중...\n",
      "검색 완료: 5 문서 찾음\n",
      "문서 기반 응답 생성 중...\n",
      "🤖 AI: 아주대학교 공과대학에는 다음과 같은 학과들이 있습니다:\n",
      "\n",
      "- 첨단신소재공학과: 첨단 반도체/디스플레이 신소재, 첨단 에너지 신소재, 첨단 경량 신소재 등 3대 핵심 전략 분야를 중심으로 교육 및 연구를 강화하고 있습니다. \n",
      "\n",
      "- AI Lab(아주혁신대학): 기존 여러 학과를 통합하여 복수의 세부 특화전공으로 구성된 학부입니다. 사회 수요를 중심으로 세부 전공을 편성하고 학생들의 전공 선택권을 보장하는 특화된 교육 체제를 운영하고 있습니다.\n",
      "\n",
      "- 프런티어과학학부: 물리학, 화학, 생명과학을 바탕으로 다양한 마이크로 전공을 선택할 수 있어 심화된 전공지식과 융합 능력을 겸비한 과학자를 양성하는 것을 목표로 합니다.\n",
      "\n",
      "이 외에도 기계공학과, 산업공학과, 화학공학과 등 다양한 전통적인 공학 분야의 학과들이 있습니다. 아주대학교 공과대학은 첨단 기술 분야와 융합 교육에 주력하고 있습니다.\n",
      "문서 기반 응답 생성 완료\n",
      "👋 채팅을 종료합니다!\n"
     ]
    }
   ],
   "source": [
    "# Format documents function\n",
    "def format_docs(docs):\n",
    "    \"\"\"Format documents into a single string\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Initialize document loading and processing\n",
    "def initialize_rag_components(file_path: str = \"./test_modified.pdf\"):\n",
    "    \"\"\"Initialize all components for RAG\"\"\"\n",
    "    print(\"문서 로딩 중...\")\n",
    "\n",
    "    # Document loading\n",
    "    loader = UpstageDocumentParseLoader(\n",
    "        file_path,\n",
    "        split=\"page\",\n",
    "        output_format=\"markdown\",\n",
    "        ocr=\"auto\",\n",
    "        coordinates=True,\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    print(f\"문서 로딩 완료: {len(docs)} 페이지\")\n",
    "\n",
    "    # Document chunking\n",
    "    print(\"문서 청킹 중...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
    "    docs_splitter = splitter.split_documents(docs)\n",
    "    print(f\"청킹 완료: {len(docs_splitter)} 청크\")\n",
    "\n",
    "    # Embedding model\n",
    "    print(\"임베딩 모델 로딩 중...\")\n",
    "    # Change 'mps' to 'cuda' for NVIDIA GPUs or 'cpu' if you don't have GPU\n",
    "    device = \"cpu\"  # 기본값으로 CPU 사용\n",
    "    try:\n",
    "        import torch\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "        elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "            device = \"mps\"\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "    print(f\"사용 중인 디바이스: {device}\")\n",
    "\n",
    "    hf_embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"intfloat/multilingual-e5-large-instruct\",\n",
    "        model_kwargs={\"device\": device},\n",
    "        encode_kwargs={\"normalize_embeddings\": True},\n",
    "    )\n",
    "\n",
    "    # Vector store\n",
    "    print(\"벡터 스토어 생성 중...\")\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents=docs_splitter,\n",
    "        embedding=hf_embeddings,\n",
    "    )\n",
    "    print(\"벡터 스토어 생성 완료\")\n",
    "\n",
    "    # Retriever\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 5},\n",
    "    )\n",
    "\n",
    "    # LLM - 🔥 스트리밍 활성화\n",
    "    print(\"LLM 초기화 중...\")\n",
    "    llm = ChatAnthropic(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        temperature=0,\n",
    "        streaming=True,  # 🔥 스트리밍 활성화\n",
    "    )\n",
    "    print(\"초기화 완료!\")\n",
    "\n",
    "    return {\n",
    "        \"retriever\": retriever,\n",
    "        \"llm\": llm,\n",
    "    }\n",
    "\n",
    "\n",
    "# 전역 변수\n",
    "rag_components = None\n",
    "conversation_history = []\n",
    "\n",
    "\n",
    "# 모델 클래스\n",
    "# Define categories for query classification\n",
    "class Category(enum.Enum):\n",
    "    DOCUMENT = \"document\"  # 문서 관련 질문\n",
    "    GENERAL = \"general\"  # 일반적인 질문\n",
    "    GREETING = \"greeting\"  # 인사말\n",
    "\n",
    "\n",
    "# Pydantic model for structured output\n",
    "class QueryClassification(BaseModel):\n",
    "    \"\"\"사용자 쿼리 분류 모델\"\"\"\n",
    "\n",
    "    category: Category = Field(\n",
    "        description=\"쿼리를 카테고리화 하세요. DOCUMENT(문서 관련 질문) / GENERAL(일반적인 질문) / GREETING(인사) 중에 하나로 구분하세요.\"\n",
    "    )\n",
    "    reasoning: str = Field(description=\"왜 이 카테고리를 선택했는지 설명하세요.\")\n",
    "\n",
    "\n",
    "# Define the state structure\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    context: List[str]\n",
    "    category: Optional[str]\n",
    "\n",
    "\n",
    "# 🔥 스트리밍 헬퍼 함수 추가\n",
    "async def stream_llm_response(llm, formatted_message):\n",
    "    \"\"\"LLM 응답을 스트리밍으로 출력하고 전체 응답 반환\"\"\"\n",
    "    print(\"🤖 AI: \", end=\"\", flush=True)\n",
    "\n",
    "    # 체인 생성\n",
    "    chain = llm | StrOutputParser()\n",
    "\n",
    "    # 스트리밍 실행\n",
    "    full_response = \"\"\n",
    "    async for chunk in chain.astream(formatted_message):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "        full_response += chunk\n",
    "\n",
    "    print()  # 줄바꿈\n",
    "    return full_response\n",
    "\n",
    "\n",
    "# Router function for categorizing queries\n",
    "def router(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"사용자 쿼리를 카테고리로 분류하는 라우터\"\"\"\n",
    "    print(\"쿼리 분류 중...\")\n",
    "\n",
    "    # Get the most recent user message\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "\n",
    "    # Create the router input\n",
    "    router_input = f\"\"\"\n",
    "    다음 사용자 쿼리를 분석하고 카테고리를 결정하세요.\n",
    "    카테고리:\n",
    "    - document: 문서 내용에 관한 질문 (예: \"아주대학교에 대해 알려줘\", \"이 문서에서 중요한 내용은?\")\n",
    "    - general: 일반적인 질문으로, 문서와 관련이 없음 (예: \"오늘 날씨 어때?\", \"파이썬이란?\")\n",
    "    - greeting: 인사말 (예: \"안녕\", \"반가워\", \"뭐해?\")\n",
    "    \n",
    "    쿼리: {user_message}\n",
    "    \"\"\"\n",
    "\n",
    "    # Get LLM\n",
    "    llm = rag_components[\"llm\"]\n",
    "\n",
    "    # Structured output with the classification model\n",
    "    structured_llm = llm.with_structured_output(QueryClassification)\n",
    "\n",
    "    # Get classification with Langfuse tracking\n",
    "    classification = structured_llm.invoke(router_input)\n",
    "\n",
    "    category = classification.category.value\n",
    "    print(f\"분류 결과: {category} (이유: {classification.reasoning})\")\n",
    "\n",
    "    return {\"category\": category}\n",
    "\n",
    "\n",
    "# Conditional routing function\n",
    "def route_by_category(state: State) -> Literal[\"document_qa\", \"general_qa\", \"greeting\"]:\n",
    "    \"\"\"카테고리에 기반하여 다음 노드를 결정\"\"\"\n",
    "    category = state.get(\"category\", \"\").lower()\n",
    "\n",
    "    if category == \"document\":\n",
    "        return \"document_qa\"\n",
    "    elif category == \"general\":\n",
    "        return \"general_qa\"\n",
    "    elif category == \"greeting\":\n",
    "        return \"greeting\"\n",
    "    else:\n",
    "        # 기본값은 일반 질의응답\n",
    "        return \"general_qa\"\n",
    "\n",
    "\n",
    "# Define LangGraph nodes\n",
    "def retrieve_documents(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"문서에서 관련 내용 검색\"\"\"\n",
    "    print(\"문서 검색 중...\")\n",
    "\n",
    "    # Get the most recent user message\n",
    "    user_message = state[\"messages\"][-1]\n",
    "\n",
    "    # Retrieve documents\n",
    "    retriever = rag_components[\"retriever\"]\n",
    "    docs = retriever.invoke(user_message.content)\n",
    "\n",
    "    # Format documents\n",
    "    formatted_docs = format_docs(docs)\n",
    "    print(f\"검색 완료: {len(docs)} 문서 찾음\")\n",
    "\n",
    "    # Return updated state\n",
    "    return {\"context\": [formatted_docs]}\n",
    "\n",
    "\n",
    "# 🔥 스트리밍 버전으로 수정\n",
    "async def document_qa(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"문서 기반 질의응답 - 스트리밍\"\"\"\n",
    "    print(\"문서 기반 응답 생성 중...\")\n",
    "    context = state[\"context\"][0] if state[\"context\"] else \"문서 정보 없음\"\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "\n",
    "    # 이전 대화들은 별도로 히스토리 구성\n",
    "    history_messages = state[\"messages\"][:-1]\n",
    "    formatted_history = \"\"\n",
    "    for msg in history_messages:\n",
    "        role = \"사용자\" if isinstance(msg, HumanMessage) else \"AI\"\n",
    "        formatted_history += f\"{role}: {msg.content}\\n\"\n",
    "\n",
    "    # Create prompt\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"너는 친절한 한국어 AI 비서야. \n",
    "        제공된 문서 내용(context)과 이전 대화 내용을 참고해서 질문에 답해.\n",
    "        반드시 한국어로만 대답하고, 문서에 없는 내용은 대답하지 말고 모른다고 해.\n",
    "        \n",
    "        참고 문서:\n",
    "        {context}\n",
    "        \n",
    "        이전 대화:\n",
    "        {chat_history}\n",
    "        \"\"\",\n",
    "            ),\n",
    "            (\"user\", \"{user_input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    llm = rag_components[\"llm\"]\n",
    "    formatted_message = prompt.format_messages(\n",
    "        context=context, chat_history=formatted_history, user_input=user_message\n",
    "    )\n",
    "\n",
    "    # 🔥 스트리밍 LLM 호출\n",
    "    response_content = await stream_llm_response(llm, formatted_message)\n",
    "    print(\"문서 기반 응답 생성 완료\")\n",
    "\n",
    "    return {\"messages\": [AIMessage(content=response_content)]}\n",
    "\n",
    "\n",
    "# 🔥 스트리밍 버전으로 수정\n",
    "async def general_qa(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"일반 질의응답 - 스트리밍\"\"\"\n",
    "    print(\"일반 응답 생성 중...\")\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "\n",
    "    # 이전 대화들은 별도로 히스토리 구성\n",
    "    history_messages = state[\"messages\"][:-1]\n",
    "    formatted_history = \"\"\n",
    "    for msg in history_messages:\n",
    "        role = \"사용자\" if isinstance(msg, HumanMessage) else \"AI\"\n",
    "        formatted_history += f\"{role}: {msg.content}\\n\"\n",
    "\n",
    "    # Create prompt\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"너는 친절한 한국어 AI 비서야. \n",
    "        사용자의 질문에 대해 간결하고 정확하게 답변해. 한국어로 대답해.\n",
    "        \n",
    "        이전 대화:\n",
    "        {chat_history}\n",
    "        \"\"\",\n",
    "            ),\n",
    "            (\"user\", \"{user_input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    llm = rag_components[\"llm\"]\n",
    "    formatted_message = prompt.format_messages(\n",
    "        user_input=user_message, chat_history=formatted_history\n",
    "    )\n",
    "\n",
    "    # 🔥 스트리밍 LLM 호출\n",
    "    response_content = await stream_llm_response(llm, formatted_message)\n",
    "    print(\"일반 응답 생성 완료\")\n",
    "\n",
    "    return {\"messages\": [AIMessage(content=response_content)]}\n",
    "\n",
    "\n",
    "# 🔥 스트리밍 버전으로 수정\n",
    "async def greeting(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"인사말에 응답 - 스트리밍\"\"\"\n",
    "    print(\"인사 응답 생성 중...\")\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "\n",
    "    # 이전 대화들은 별도로 히스토리 구성\n",
    "    history_messages = state[\"messages\"][:-1]\n",
    "    formatted_history = \"\"\n",
    "    for msg in history_messages:\n",
    "        role = \"사용자\" if isinstance(msg, HumanMessage) else \"AI\"\n",
    "        formatted_history += f\"{role}: {msg.content}\\n\"\n",
    "\n",
    "    # Create prompt\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"너는 친절한 한국어 AI 비서야.\n",
    "        사용자의 인사에 친근하고 따뜻하게 응답해. 간결하게 한국어로 대답해.\n",
    "\n",
    "        이전 대화:\n",
    "        {chat_history}\n",
    "        \"\"\",\n",
    "            ),\n",
    "            (\"user\", \"{user_input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    llm = rag_components[\"llm\"]\n",
    "    formatted_message = prompt.format_messages(\n",
    "        user_input=user_message, chat_history=formatted_history\n",
    "    )\n",
    "\n",
    "    # 🔥 스트리밍 LLM 호출\n",
    "    response_content = await stream_llm_response(llm, formatted_message)\n",
    "    print(\"인사 응답 생성 완료\")\n",
    "\n",
    "    return {\"messages\": [AIMessage(content=response_content)]}\n",
    "\n",
    "\n",
    "# 🔥 비동기 그래프 실행 함수\n",
    "async def run_graph_streaming(user_input: str):\n",
    "    \"\"\"스트리밍으로 그래프 실행\"\"\"\n",
    "    global conversation_history\n",
    "\n",
    "    user_message = HumanMessage(content=user_input)\n",
    "    conversation_history.append(user_message)\n",
    "\n",
    "    initial_state = {\n",
    "        \"messages\": conversation_history,\n",
    "        \"context\": [],\n",
    "        \"category\": None,\n",
    "    }\n",
    "\n",
    "    # 스트리밍 그래프 빌드\n",
    "    graph_builder = StateGraph(State)\n",
    "\n",
    "    # 동기 노드들\n",
    "    graph_builder.add_node(\"router\", router)\n",
    "    graph_builder.add_node(\"retrieve\", retrieve_documents)\n",
    "\n",
    "    # 🔥 비동기 스트리밍 노드들\n",
    "    graph_builder.add_node(\"document_qa\", document_qa)\n",
    "    graph_builder.add_node(\"general_qa\", general_qa)\n",
    "    graph_builder.add_node(\"greeting\", greeting)\n",
    "\n",
    "    # 엣지 연결\n",
    "    graph_builder.add_edge(START, \"router\")\n",
    "    graph_builder.add_conditional_edges(\n",
    "        \"router\",\n",
    "        route_by_category,\n",
    "        {\n",
    "            \"document_qa\": \"retrieve\",\n",
    "            \"general_qa\": \"general_qa\",\n",
    "            \"greeting\": \"greeting\",\n",
    "        },\n",
    "    )\n",
    "    graph_builder.add_edge(\"retrieve\", \"document_qa\")\n",
    "    graph_builder.add_edge(\"document_qa\", END)\n",
    "    graph_builder.add_edge(\"general_qa\", END)\n",
    "    graph_builder.add_edge(\"greeting\", END)\n",
    "\n",
    "    # 그래프 컴파일\n",
    "    streaming_graph = graph_builder.compile()\n",
    "\n",
    "    # 🔥 비동기 실행\n",
    "    result = await streaming_graph.ainvoke(initial_state)\n",
    "\n",
    "    # AI 응답을 히스토리에 추가\n",
    "    if \"messages\" in result and len(result[\"messages\"]) > 1:\n",
    "        ai_msg = result[\"messages\"][-1]\n",
    "        if isinstance(ai_msg, AIMessage):\n",
    "            conversation_history.append(ai_msg)\n",
    "            return ai_msg.content\n",
    "\n",
    "    return \"응답을 생성할 수 없습니다.\"\n",
    "\n",
    "\n",
    "# 🔥 비동기 채팅 인터페이스\n",
    "async def interactive_chat_streaming():\n",
    "    \"\"\"스트리밍 채팅 인터페이스\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"🤖 LangGraph 라우팅 RAG 챗봇 시작 (스트리밍 지원)\")\n",
    "    print(\"💡 '종료' 입력 시 대화를 끝냅니다.\")\n",
    "    print(\"💡 '히스토리' 입력 시 현재 대화 히스토리를 확인합니다.\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"\\n🙋 사용자: \").strip()\n",
    "\n",
    "                if not user_input:\n",
    "                    print(\"❗ 메시지를 입력해주세요.\")\n",
    "                    continue\n",
    "\n",
    "                if user_input.lower() == \"종료\":\n",
    "                    print(\"👋 채팅을 종료합니다!\")\n",
    "                    break\n",
    "\n",
    "                if user_input.lower() == \"히스토리\":\n",
    "                    print(\"\\n=== 현재 대화 히스토리 ===\")\n",
    "                    for i, msg in enumerate(conversation_history):\n",
    "                        msg_type = \"사용자\" if isinstance(msg, HumanMessage) else \"AI\"\n",
    "                        content = (\n",
    "                            msg.content[:100] + \"...\"\n",
    "                            if len(msg.content) > 100\n",
    "                            else msg.content\n",
    "                        )\n",
    "                        print(f\"{i+1}. {msg_type}: {content}\")\n",
    "                    print(f\"총 {len(conversation_history)}개 메시지\")\n",
    "                    print(\"=\" * 30)\n",
    "                    continue\n",
    "\n",
    "                # 🔥 스트리밍으로 응답 생성\n",
    "                await run_graph_streaming(user_input)\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n👋 채팅을 종료합니다!\")\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {str(e)}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "# 🔥 메인 실행 함수\n",
    "async def main_streaming():\n",
    "    \"\"\"메인 실행 함수\"\"\"\n",
    "    global rag_components, conversation_history\n",
    "\n",
    "    try:\n",
    "        # RAG 컴포넌트 초기화\n",
    "        print(\"RAG 컴포넌트 초기화 중...\")\n",
    "        rag_components = initialize_rag_components()\n",
    "        conversation_history = []\n",
    "\n",
    "        print(\"🚀 스트리밍 RAG 챗봇 시작!\")\n",
    "\n",
    "        # 스트리밍 채팅 시작\n",
    "        await interactive_chat_streaming()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"초기화 중 오류 발생: {str(e)}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "# 실행 방법\n",
    "# Jupyter Notebook에서:\n",
    "await main_streaming()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecae2925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
