{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bcab1d9",
   "metadata": {},
   "source": [
    "ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed375125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Optional, Literal, List, Dict, Any\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "import enum\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import uuid\n",
    "\n",
    "# LangChain ë° ê¸°íƒ€ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "\n",
    "# LangGraph ê´€ë ¨ ì„í¬íŠ¸\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# ê¸°ì¡´ RAG ì½”ë“œì—ì„œ ê°€ì ¸ì˜¨ ì»´í¬ë„ŒíŠ¸\n",
    "from langchain_upstage import UpstageDocumentParseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ë­í“¨ì¦ˆ\n",
    "from langfuse import Langfuse, get_client\n",
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# ìŠ¤íŠ¸ë¦¬ë°\n",
    "import asyncio\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a8265c",
   "metadata": {},
   "source": [
    "ì„¤ì • ë° ì´ˆê¸°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edac3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .env íŒŒì¼ ë¡œë“œ (í•„ìš”í•œ ê²½ìš°)\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4087a03a",
   "metadata": {},
   "source": [
    "ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32285bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì¤‘...\n",
      "ë¬¸ì„œ ë¡œë”© ì¤‘...\n",
      "ë¬¸ì„œ ë¡œë”© ì™„ë£Œ: 40 í˜ì´ì§€\n",
      "ë¬¸ì„œ ì²­í‚¹ ì¤‘...\n",
      "ì²­í‚¹ ì™„ë£Œ: 70 ì²­í¬\n",
      "ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...\n",
      "ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: mps\n",
      "ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘...\n",
      "ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ\n",
      "LLM ì´ˆê¸°í™” ì¤‘...\n",
      "ì´ˆê¸°í™” ì™„ë£Œ!\n",
      "ğŸš€ ìŠ¤íŠ¸ë¦¬ë° RAG ì±—ë´‡ ì‹œì‘!\n",
      "============================================================\n",
      "ğŸ¤– LangGraph ë¼ìš°íŒ… RAG ì±—ë´‡ ì‹œì‘ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)\n",
      "ğŸ’¡ 'ì¢…ë£Œ' ì…ë ¥ ì‹œ ëŒ€í™”ë¥¼ ëëƒ…ë‹ˆë‹¤.\n",
      "ğŸ’¡ 'íˆìŠ¤í† ë¦¬' ì…ë ¥ ì‹œ í˜„ì¬ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
      "============================================================\n",
      "ì¿¼ë¦¬ ë¶„ë¥˜ ì¤‘...\n",
      "ë¶„ë¥˜ ê²°ê³¼: document (ì´ìœ : ì´ ì¿¼ë¦¬ëŠ” ì•„ì£¼ëŒ€í•™êµ ê³µê³¼ëŒ€í•™ì— ëŒ€í•œ ì •ë³´ë¥¼ ìš”ì²­í•˜ê³  ìˆìœ¼ë¯€ë¡œ document ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•©ë‹ˆë‹¤.)\n",
      "ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...\n",
      "ê²€ìƒ‰ ì™„ë£Œ: 5 ë¬¸ì„œ ì°¾ìŒ\n",
      "ë¬¸ì„œ ê¸°ë°˜ ì‘ë‹µ ìƒì„± ì¤‘...\n",
      "ğŸ¤– AI: ì•„ì£¼ëŒ€í•™êµ ê³µê³¼ëŒ€í•™ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ í•™ê³¼ë“¤ì´ ìˆìŠµë‹ˆë‹¤:\n",
      "\n",
      "- ì²¨ë‹¨ì‹ ì†Œì¬ê³µí•™ê³¼: ì²¨ë‹¨ ë°˜ë„ì²´/ë””ìŠ¤í”Œë ˆì´ ì‹ ì†Œì¬, ì²¨ë‹¨ ì—ë„ˆì§€ ì‹ ì†Œì¬, ì²¨ë‹¨ ê²½ëŸ‰ ì‹ ì†Œì¬ ë“± 3ëŒ€ í•µì‹¬ ì „ëµ ë¶„ì•¼ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ êµìœ¡ ë° ì—°êµ¬ë¥¼ ê°•í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤. \n",
      "\n",
      "- AI Lab(ì•„ì£¼í˜ì‹ ëŒ€í•™): ê¸°ì¡´ ì—¬ëŸ¬ í•™ê³¼ë¥¼ í†µí•©í•˜ì—¬ ë³µìˆ˜ì˜ ì„¸ë¶€ íŠ¹í™”ì „ê³µìœ¼ë¡œ êµ¬ì„±ëœ í•™ë¶€ì…ë‹ˆë‹¤. ì‚¬íšŒ ìˆ˜ìš”ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì„¸ë¶€ ì „ê³µì„ í¸ì„±í•˜ê³  í•™ìƒë“¤ì˜ ì „ê³µ ì„ íƒê¶Œì„ ë³´ì¥í•˜ëŠ” íŠ¹í™”ëœ êµìœ¡ ì²´ì œë¥¼ ìš´ì˜í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "- í”„ëŸ°í‹°ì–´ê³¼í•™í•™ë¶€: ë¬¼ë¦¬í•™, í™”í•™, ìƒëª…ê³¼í•™ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ì–‘í•œ ë§ˆì´í¬ë¡œ ì „ê³µì„ ì„ íƒí•  ìˆ˜ ìˆì–´ ì‹¬í™”ëœ ì „ê³µì§€ì‹ê³¼ ìœµí•© ëŠ¥ë ¥ì„ ê²¸ë¹„í•œ ê³¼í•™ìë¥¼ ì–‘ì„±í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.\n",
      "\n",
      "ì´ ì™¸ì—ë„ ê¸°ê³„ê³µí•™ê³¼, ì‚°ì—…ê³µí•™ê³¼, í™”í•™ê³µí•™ê³¼ ë“± ë‹¤ì–‘í•œ ì „í†µì ì¸ ê³µí•™ ë¶„ì•¼ì˜ í•™ê³¼ë“¤ì´ ìˆìŠµë‹ˆë‹¤. ì•„ì£¼ëŒ€í•™êµ ê³µê³¼ëŒ€í•™ì€ ì²¨ë‹¨ ê¸°ìˆ  ë¶„ì•¼ì™€ ìœµí•© êµìœ¡ì— ì£¼ë ¥í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "ë¬¸ì„œ ê¸°ë°˜ ì‘ë‹µ ìƒì„± ì™„ë£Œ\n",
      "ğŸ‘‹ ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "# Format documents function\n",
    "def format_docs(docs):\n",
    "    \"\"\"Format documents into a single string\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Initialize document loading and processing\n",
    "def initialize_rag_components(file_path: str = \"./test_modified.pdf\"):\n",
    "    \"\"\"Initialize all components for RAG\"\"\"\n",
    "    print(\"ë¬¸ì„œ ë¡œë”© ì¤‘...\")\n",
    "\n",
    "    # Document loading\n",
    "    loader = UpstageDocumentParseLoader(\n",
    "        file_path,\n",
    "        split=\"page\",\n",
    "        output_format=\"markdown\",\n",
    "        ocr=\"auto\",\n",
    "        coordinates=True,\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    print(f\"ë¬¸ì„œ ë¡œë”© ì™„ë£Œ: {len(docs)} í˜ì´ì§€\")\n",
    "\n",
    "    # Document chunking\n",
    "    print(\"ë¬¸ì„œ ì²­í‚¹ ì¤‘...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
    "    docs_splitter = splitter.split_documents(docs)\n",
    "    print(f\"ì²­í‚¹ ì™„ë£Œ: {len(docs_splitter)} ì²­í¬\")\n",
    "\n",
    "    # Embedding model\n",
    "    print(\"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "    # Change 'mps' to 'cuda' for NVIDIA GPUs or 'cpu' if you don't have GPU\n",
    "    device = \"cpu\"  # ê¸°ë³¸ê°’ìœ¼ë¡œ CPU ì‚¬ìš©\n",
    "    try:\n",
    "        import torch\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "        elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "            device = \"mps\"\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "    print(f\"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "    hf_embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"intfloat/multilingual-e5-large-instruct\",\n",
    "        model_kwargs={\"device\": device},\n",
    "        encode_kwargs={\"normalize_embeddings\": True},\n",
    "    )\n",
    "\n",
    "    # Vector store\n",
    "    print(\"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘...\")\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents=docs_splitter,\n",
    "        embedding=hf_embeddings,\n",
    "    )\n",
    "    print(\"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "    # Retriever\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 5},\n",
    "    )\n",
    "\n",
    "    # LLM - ğŸ”¥ ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”\n",
    "    print(\"LLM ì´ˆê¸°í™” ì¤‘...\")\n",
    "    llm = ChatAnthropic(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        temperature=0,\n",
    "        streaming=True,  # ğŸ”¥ ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”\n",
    "    )\n",
    "    print(\"ì´ˆê¸°í™” ì™„ë£Œ!\")\n",
    "\n",
    "    return {\n",
    "        \"retriever\": retriever,\n",
    "        \"llm\": llm,\n",
    "    }\n",
    "\n",
    "\n",
    "# ì „ì—­ ë³€ìˆ˜\n",
    "rag_components = None\n",
    "conversation_history = []\n",
    "\n",
    "\n",
    "# ëª¨ë¸ í´ë˜ìŠ¤\n",
    "# Define categories for query classification\n",
    "class Category(enum.Enum):\n",
    "    DOCUMENT = \"document\"  # ë¬¸ì„œ ê´€ë ¨ ì§ˆë¬¸\n",
    "    GENERAL = \"general\"  # ì¼ë°˜ì ì¸ ì§ˆë¬¸\n",
    "    GREETING = \"greeting\"  # ì¸ì‚¬ë§\n",
    "\n",
    "\n",
    "# Pydantic model for structured output\n",
    "class QueryClassification(BaseModel):\n",
    "    \"\"\"ì‚¬ìš©ì ì¿¼ë¦¬ ë¶„ë¥˜ ëª¨ë¸\"\"\"\n",
    "\n",
    "    category: Category = Field(\n",
    "        description=\"ì¿¼ë¦¬ë¥¼ ì¹´í…Œê³ ë¦¬í™” í•˜ì„¸ìš”. DOCUMENT(ë¬¸ì„œ ê´€ë ¨ ì§ˆë¬¸) / GENERAL(ì¼ë°˜ì ì¸ ì§ˆë¬¸) / GREETING(ì¸ì‚¬) ì¤‘ì— í•˜ë‚˜ë¡œ êµ¬ë¶„í•˜ì„¸ìš”.\"\n",
    "    )\n",
    "    reasoning: str = Field(description=\"ì™œ ì´ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí–ˆëŠ”ì§€ ì„¤ëª…í•˜ì„¸ìš”.\")\n",
    "\n",
    "\n",
    "# Define the state structure\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    context: List[str]\n",
    "    category: Optional[str]\n",
    "\n",
    "\n",
    "# ğŸ”¥ ìŠ¤íŠ¸ë¦¬ë° í—¬í¼ í•¨ìˆ˜ ì¶”ê°€\n",
    "async def stream_llm_response(llm, formatted_message):\n",
    "    \"\"\"LLM ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì¶œë ¥í•˜ê³  ì „ì²´ ì‘ë‹µ ë°˜í™˜\"\"\"\n",
    "    print(\"ğŸ¤– AI: \", end=\"\", flush=True)\n",
    "\n",
    "    # ì²´ì¸ ìƒì„±\n",
    "    chain = llm | StrOutputParser()\n",
    "\n",
    "    # ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰\n",
    "    full_response = \"\"\n",
    "    async for chunk in chain.astream(formatted_message):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "        full_response += chunk\n",
    "\n",
    "    print()  # ì¤„ë°”ê¿ˆ\n",
    "    return full_response\n",
    "\n",
    "\n",
    "# Router function for categorizing queries\n",
    "def router(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ëŠ” ë¼ìš°í„°\"\"\"\n",
    "    print(\"ì¿¼ë¦¬ ë¶„ë¥˜ ì¤‘...\")\n",
    "\n",
    "    # Get the most recent user message\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "\n",
    "    # Create the router input\n",
    "    router_input = f\"\"\"\n",
    "    ë‹¤ìŒ ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ê³  ì¹´í…Œê³ ë¦¬ë¥¼ ê²°ì •í•˜ì„¸ìš”.\n",
    "    ì¹´í…Œê³ ë¦¬:\n",
    "    - document: ë¬¸ì„œ ë‚´ìš©ì— ê´€í•œ ì§ˆë¬¸ (ì˜ˆ: \"ì•„ì£¼ëŒ€í•™êµì— ëŒ€í•´ ì•Œë ¤ì¤˜\", \"ì´ ë¬¸ì„œì—ì„œ ì¤‘ìš”í•œ ë‚´ìš©ì€?\")\n",
    "    - general: ì¼ë°˜ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ, ë¬¸ì„œì™€ ê´€ë ¨ì´ ì—†ìŒ (ì˜ˆ: \"ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?\", \"íŒŒì´ì¬ì´ë€?\")\n",
    "    - greeting: ì¸ì‚¬ë§ (ì˜ˆ: \"ì•ˆë…•\", \"ë°˜ê°€ì›Œ\", \"ë­í•´?\")\n",
    "    \n",
    "    ì¿¼ë¦¬: {user_message}\n",
    "    \"\"\"\n",
    "\n",
    "    # Get LLM\n",
    "    llm = rag_components[\"llm\"]\n",
    "\n",
    "    # Structured output with the classification model\n",
    "    structured_llm = llm.with_structured_output(QueryClassification)\n",
    "\n",
    "    # Get classification with Langfuse tracking\n",
    "    classification = structured_llm.invoke(router_input)\n",
    "\n",
    "    category = classification.category.value\n",
    "    print(f\"ë¶„ë¥˜ ê²°ê³¼: {category} (ì´ìœ : {classification.reasoning})\")\n",
    "\n",
    "    return {\"category\": category}\n",
    "\n",
    "\n",
    "# Conditional routing function\n",
    "def route_by_category(state: State) -> Literal[\"document_qa\", \"general_qa\", \"greeting\"]:\n",
    "    \"\"\"ì¹´í…Œê³ ë¦¬ì— ê¸°ë°˜í•˜ì—¬ ë‹¤ìŒ ë…¸ë“œë¥¼ ê²°ì •\"\"\"\n",
    "    category = state.get(\"category\", \"\").lower()\n",
    "\n",
    "    if category == \"document\":\n",
    "        return \"document_qa\"\n",
    "    elif category == \"general\":\n",
    "        return \"general_qa\"\n",
    "    elif category == \"greeting\":\n",
    "        return \"greeting\"\n",
    "    else:\n",
    "        # ê¸°ë³¸ê°’ì€ ì¼ë°˜ ì§ˆì˜ì‘ë‹µ\n",
    "        return \"general_qa\"\n",
    "\n",
    "\n",
    "# Define LangGraph nodes\n",
    "def retrieve_documents(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš© ê²€ìƒ‰\"\"\"\n",
    "    print(\"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...\")\n",
    "\n",
    "    # Get the most recent user message\n",
    "    user_message = state[\"messages\"][-1]\n",
    "\n",
    "    # Retrieve documents\n",
    "    retriever = rag_components[\"retriever\"]\n",
    "    docs = retriever.invoke(user_message.content)\n",
    "\n",
    "    # Format documents\n",
    "    formatted_docs = format_docs(docs)\n",
    "    print(f\"ê²€ìƒ‰ ì™„ë£Œ: {len(docs)} ë¬¸ì„œ ì°¾ìŒ\")\n",
    "\n",
    "    # Return updated state\n",
    "    return {\"context\": [formatted_docs]}\n",
    "\n",
    "\n",
    "# ğŸ”¥ ìŠ¤íŠ¸ë¦¬ë° ë²„ì „ìœ¼ë¡œ ìˆ˜ì •\n",
    "async def document_qa(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ - ìŠ¤íŠ¸ë¦¬ë°\"\"\"\n",
    "    print(\"ë¬¸ì„œ ê¸°ë°˜ ì‘ë‹µ ìƒì„± ì¤‘...\")\n",
    "    context = state[\"context\"][0] if state[\"context\"] else \"ë¬¸ì„œ ì •ë³´ ì—†ìŒ\"\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "\n",
    "    # ì´ì „ ëŒ€í™”ë“¤ì€ ë³„ë„ë¡œ íˆìŠ¤í† ë¦¬ êµ¬ì„±\n",
    "    history_messages = state[\"messages\"][:-1]\n",
    "    formatted_history = \"\"\n",
    "    for msg in history_messages:\n",
    "        role = \"ì‚¬ìš©ì\" if isinstance(msg, HumanMessage) else \"AI\"\n",
    "        formatted_history += f\"{role}: {msg.content}\\n\"\n",
    "\n",
    "    # Create prompt\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"ë„ˆëŠ” ì¹œì ˆí•œ í•œêµ­ì–´ AI ë¹„ì„œì•¼. \n",
    "        ì œê³µëœ ë¬¸ì„œ ë‚´ìš©(context)ê³¼ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µí•´.\n",
    "        ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ëŒ€ë‹µí•˜ê³ , ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ëŒ€ë‹µí•˜ì§€ ë§ê³  ëª¨ë¥¸ë‹¤ê³  í•´.\n",
    "        \n",
    "        ì°¸ê³  ë¬¸ì„œ:\n",
    "        {context}\n",
    "        \n",
    "        ì´ì „ ëŒ€í™”:\n",
    "        {chat_history}\n",
    "        \"\"\",\n",
    "            ),\n",
    "            (\"user\", \"{user_input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    llm = rag_components[\"llm\"]\n",
    "    formatted_message = prompt.format_messages(\n",
    "        context=context, chat_history=formatted_history, user_input=user_message\n",
    "    )\n",
    "\n",
    "    # ğŸ”¥ ìŠ¤íŠ¸ë¦¬ë° LLM í˜¸ì¶œ\n",
    "    response_content = await stream_llm_response(llm, formatted_message)\n",
    "    print(\"ë¬¸ì„œ ê¸°ë°˜ ì‘ë‹µ ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "    return {\"messages\": [AIMessage(content=response_content)]}\n",
    "\n",
    "\n",
    "# ğŸ”¥ ìŠ¤íŠ¸ë¦¬ë° ë²„ì „ìœ¼ë¡œ ìˆ˜ì •\n",
    "async def general_qa(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"ì¼ë°˜ ì§ˆì˜ì‘ë‹µ - ìŠ¤íŠ¸ë¦¬ë°\"\"\"\n",
    "    print(\"ì¼ë°˜ ì‘ë‹µ ìƒì„± ì¤‘...\")\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "\n",
    "    # ì´ì „ ëŒ€í™”ë“¤ì€ ë³„ë„ë¡œ íˆìŠ¤í† ë¦¬ êµ¬ì„±\n",
    "    history_messages = state[\"messages\"][:-1]\n",
    "    formatted_history = \"\"\n",
    "    for msg in history_messages:\n",
    "        role = \"ì‚¬ìš©ì\" if isinstance(msg, HumanMessage) else \"AI\"\n",
    "        formatted_history += f\"{role}: {msg.content}\\n\"\n",
    "\n",
    "    # Create prompt\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"ë„ˆëŠ” ì¹œì ˆí•œ í•œêµ­ì–´ AI ë¹„ì„œì•¼. \n",
    "        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´. í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•´.\n",
    "        \n",
    "        ì´ì „ ëŒ€í™”:\n",
    "        {chat_history}\n",
    "        \"\"\",\n",
    "            ),\n",
    "            (\"user\", \"{user_input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    llm = rag_components[\"llm\"]\n",
    "    formatted_message = prompt.format_messages(\n",
    "        user_input=user_message, chat_history=formatted_history\n",
    "    )\n",
    "\n",
    "    # ğŸ”¥ ìŠ¤íŠ¸ë¦¬ë° LLM í˜¸ì¶œ\n",
    "    response_content = await stream_llm_response(llm, formatted_message)\n",
    "    print(\"ì¼ë°˜ ì‘ë‹µ ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "    return {\"messages\": [AIMessage(content=response_content)]}\n",
    "\n",
    "\n",
    "# ğŸ”¥ ìŠ¤íŠ¸ë¦¬ë° ë²„ì „ìœ¼ë¡œ ìˆ˜ì •\n",
    "async def greeting(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"ì¸ì‚¬ë§ì— ì‘ë‹µ - ìŠ¤íŠ¸ë¦¬ë°\"\"\"\n",
    "    print(\"ì¸ì‚¬ ì‘ë‹µ ìƒì„± ì¤‘...\")\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "\n",
    "    # ì´ì „ ëŒ€í™”ë“¤ì€ ë³„ë„ë¡œ íˆìŠ¤í† ë¦¬ êµ¬ì„±\n",
    "    history_messages = state[\"messages\"][:-1]\n",
    "    formatted_history = \"\"\n",
    "    for msg in history_messages:\n",
    "        role = \"ì‚¬ìš©ì\" if isinstance(msg, HumanMessage) else \"AI\"\n",
    "        formatted_history += f\"{role}: {msg.content}\\n\"\n",
    "\n",
    "    # Create prompt\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"ë„ˆëŠ” ì¹œì ˆí•œ í•œêµ­ì–´ AI ë¹„ì„œì•¼.\n",
    "        ì‚¬ìš©ìì˜ ì¸ì‚¬ì— ì¹œê·¼í•˜ê³  ë”°ëœ»í•˜ê²Œ ì‘ë‹µí•´. ê°„ê²°í•˜ê²Œ í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•´.\n",
    "\n",
    "        ì´ì „ ëŒ€í™”:\n",
    "        {chat_history}\n",
    "        \"\"\",\n",
    "            ),\n",
    "            (\"user\", \"{user_input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    llm = rag_components[\"llm\"]\n",
    "    formatted_message = prompt.format_messages(\n",
    "        user_input=user_message, chat_history=formatted_history\n",
    "    )\n",
    "\n",
    "    # ğŸ”¥ ìŠ¤íŠ¸ë¦¬ë° LLM í˜¸ì¶œ\n",
    "    response_content = await stream_llm_response(llm, formatted_message)\n",
    "    print(\"ì¸ì‚¬ ì‘ë‹µ ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "    return {\"messages\": [AIMessage(content=response_content)]}\n",
    "\n",
    "\n",
    "# ğŸ”¥ ë¹„ë™ê¸° ê·¸ë˜í”„ ì‹¤í–‰ í•¨ìˆ˜\n",
    "async def run_graph_streaming(user_input: str):\n",
    "    \"\"\"ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ê·¸ë˜í”„ ì‹¤í–‰\"\"\"\n",
    "    global conversation_history\n",
    "\n",
    "    user_message = HumanMessage(content=user_input)\n",
    "    conversation_history.append(user_message)\n",
    "\n",
    "    initial_state = {\n",
    "        \"messages\": conversation_history,\n",
    "        \"context\": [],\n",
    "        \"category\": None,\n",
    "    }\n",
    "\n",
    "    # ìŠ¤íŠ¸ë¦¬ë° ê·¸ë˜í”„ ë¹Œë“œ\n",
    "    graph_builder = StateGraph(State)\n",
    "\n",
    "    # ë™ê¸° ë…¸ë“œë“¤\n",
    "    graph_builder.add_node(\"router\", router)\n",
    "    graph_builder.add_node(\"retrieve\", retrieve_documents)\n",
    "\n",
    "    # ğŸ”¥ ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ë…¸ë“œë“¤\n",
    "    graph_builder.add_node(\"document_qa\", document_qa)\n",
    "    graph_builder.add_node(\"general_qa\", general_qa)\n",
    "    graph_builder.add_node(\"greeting\", greeting)\n",
    "\n",
    "    # ì—£ì§€ ì—°ê²°\n",
    "    graph_builder.add_edge(START, \"router\")\n",
    "    graph_builder.add_conditional_edges(\n",
    "        \"router\",\n",
    "        route_by_category,\n",
    "        {\n",
    "            \"document_qa\": \"retrieve\",\n",
    "            \"general_qa\": \"general_qa\",\n",
    "            \"greeting\": \"greeting\",\n",
    "        },\n",
    "    )\n",
    "    graph_builder.add_edge(\"retrieve\", \"document_qa\")\n",
    "    graph_builder.add_edge(\"document_qa\", END)\n",
    "    graph_builder.add_edge(\"general_qa\", END)\n",
    "    graph_builder.add_edge(\"greeting\", END)\n",
    "\n",
    "    # ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "    streaming_graph = graph_builder.compile()\n",
    "\n",
    "    # ğŸ”¥ ë¹„ë™ê¸° ì‹¤í–‰\n",
    "    result = await streaming_graph.ainvoke(initial_state)\n",
    "\n",
    "    # AI ì‘ë‹µì„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€\n",
    "    if \"messages\" in result and len(result[\"messages\"]) > 1:\n",
    "        ai_msg = result[\"messages\"][-1]\n",
    "        if isinstance(ai_msg, AIMessage):\n",
    "            conversation_history.append(ai_msg)\n",
    "            return ai_msg.content\n",
    "\n",
    "    return \"ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "\n",
    "# ğŸ”¥ ë¹„ë™ê¸° ì±„íŒ… ì¸í„°í˜ì´ìŠ¤\n",
    "async def interactive_chat_streaming():\n",
    "    \"\"\"ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì¸í„°í˜ì´ìŠ¤\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ¤– LangGraph ë¼ìš°íŒ… RAG ì±—ë´‡ ì‹œì‘ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)\")\n",
    "    print(\"ğŸ’¡ 'ì¢…ë£Œ' ì…ë ¥ ì‹œ ëŒ€í™”ë¥¼ ëëƒ…ë‹ˆë‹¤.\")\n",
    "    print(\"ğŸ’¡ 'íˆìŠ¤í† ë¦¬' ì…ë ¥ ì‹œ í˜„ì¬ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"\\nğŸ™‹ ì‚¬ìš©ì: \").strip()\n",
    "\n",
    "                if not user_input:\n",
    "                    print(\"â— ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\")\n",
    "                    continue\n",
    "\n",
    "                if user_input.lower() == \"ì¢…ë£Œ\":\n",
    "                    print(\"ğŸ‘‹ ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤!\")\n",
    "                    break\n",
    "\n",
    "                if user_input.lower() == \"íˆìŠ¤í† ë¦¬\":\n",
    "                    print(\"\\n=== í˜„ì¬ ëŒ€í™” íˆìŠ¤í† ë¦¬ ===\")\n",
    "                    for i, msg in enumerate(conversation_history):\n",
    "                        msg_type = \"ì‚¬ìš©ì\" if isinstance(msg, HumanMessage) else \"AI\"\n",
    "                        content = (\n",
    "                            msg.content[:100] + \"...\"\n",
    "                            if len(msg.content) > 100\n",
    "                            else msg.content\n",
    "                        )\n",
    "                        print(f\"{i+1}. {msg_type}: {content}\")\n",
    "                    print(f\"ì´ {len(conversation_history)}ê°œ ë©”ì‹œì§€\")\n",
    "                    print(\"=\" * 30)\n",
    "                    continue\n",
    "\n",
    "                # ğŸ”¥ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‘ë‹µ ìƒì„±\n",
    "                await run_graph_streaming(user_input)\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nğŸ‘‹ ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤!\")\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "# ğŸ”¥ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\n",
    "async def main_streaming():\n",
    "    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n",
    "    global rag_components, conversation_history\n",
    "\n",
    "    try:\n",
    "        # RAG ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”\n",
    "        print(\"RAG ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì¤‘...\")\n",
    "        rag_components = initialize_rag_components()\n",
    "        conversation_history = []\n",
    "\n",
    "        print(\"ğŸš€ ìŠ¤íŠ¸ë¦¬ë° RAG ì±—ë´‡ ì‹œì‘!\")\n",
    "\n",
    "        # ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì‹œì‘\n",
    "        await interactive_chat_streaming()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "# ì‹¤í–‰ ë°©ë²•\n",
    "# Jupyter Notebookì—ì„œ:\n",
    "await main_streaming()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecae2925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
