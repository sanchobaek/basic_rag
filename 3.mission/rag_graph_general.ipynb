{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d9a1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baegjonghun/.pyenv/versions/3.11.10/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Annotated, Optional, Literal, List, Dict, Any\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "import enum\n",
    "\n",
    "# LangChain ë° ê¸°íƒ€ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "# LangGraph ê´€ë ¨ ì„í¬íŠ¸\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# ê¸°ì¡´ RAG ì½”ë“œì—ì„œ ê°€ì ¸ì˜¨ ì»´í¬ë„ŒíŠ¸\n",
    "from langchain_upstage import UpstageDocumentParseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59fa30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .env íŒŒì¼ ë¡œë“œ (í•„ìš”í•œ ê²½ìš°)\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0640931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Dict, List, Any, Optional, Literal\n",
    "from typing_extensions import TypedDict\n",
    "import os\n",
    "import time\n",
    "import enum\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_upstage import UpstageDocumentParseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Format documents function\n",
    "def format_docs(docs):\n",
    "    \"\"\"Format documents into a single string\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Initialize document loading and processing\n",
    "def initialize_rag_components(file_path: str = \"./test_modified.pdf\"):\n",
    "    \"\"\"Initialize all components for RAG\"\"\"\n",
    "    print(\"ë¬¸ì„œ ë¡œë”© ì¤‘...\")\n",
    "\n",
    "    # Document loading\n",
    "    loader = UpstageDocumentParseLoader(\n",
    "        file_path,\n",
    "        split=\"page\",\n",
    "        output_format=\"markdown\",\n",
    "        ocr=\"auto\",\n",
    "        coordinates=True,\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    print(f\"ë¬¸ì„œ ë¡œë”© ì™„ë£Œ: {len(docs)} í˜ì´ì§€\")\n",
    "\n",
    "    # Document chunking\n",
    "    print(\"ë¬¸ì„œ ì²­í‚¹ ì¤‘...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
    "    docs_splitter = splitter.split_documents(docs)\n",
    "    print(f\"ì²­í‚¹ ì™„ë£Œ: {len(docs_splitter)} ì²­í¬\")\n",
    "\n",
    "    # Embedding model\n",
    "    print(\"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "    # Change 'mps' to 'cuda' for NVIDIA GPUs or 'cpu' if you don't have GPU\n",
    "    device = \"cpu\"  # ê¸°ë³¸ê°’ìœ¼ë¡œ CPU ì‚¬ìš©\n",
    "    try:\n",
    "        import torch\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "        elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "            device = \"mps\"\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "    print(f\"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "    hf_embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"intfloat/multilingual-e5-large-instruct\",\n",
    "        model_kwargs={\"device\": device},\n",
    "        encode_kwargs={\"normalize_embeddings\": True},\n",
    "    )\n",
    "\n",
    "    # Vector store\n",
    "    print(\"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘...\")\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents=docs_splitter,\n",
    "        embedding=hf_embeddings,\n",
    "    )\n",
    "    print(\"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "    # Retriever\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 5},\n",
    "    )\n",
    "\n",
    "    # LLM\n",
    "    print(\"LLM ì´ˆê¸°í™” ì¤‘...\")\n",
    "    llm = ChatAnthropic(model=\"claude-3-haiku-20240307\", temperature=0)\n",
    "    print(\"ì´ˆê¸°í™” ì™„ë£Œ!\")\n",
    "\n",
    "    return {\n",
    "        \"retriever\": retriever,\n",
    "        \"llm\": llm,\n",
    "    }\n",
    "\n",
    "\n",
    "# Define categories for query classification\n",
    "class Category(enum.Enum):\n",
    "    DOCUMENT = \"document\"  # ë¬¸ì„œ ê´€ë ¨ ì§ˆë¬¸\n",
    "    GENERAL = \"general\"  # ì¼ë°˜ì ì¸ ì§ˆë¬¸\n",
    "    GREETING = \"greeting\"  # ì¸ì‚¬ë§\n",
    "\n",
    "\n",
    "# Pydantic model for structured output\n",
    "class QueryClassification(BaseModel):\n",
    "    \"\"\"ì‚¬ìš©ì ì¿¼ë¦¬ ë¶„ë¥˜ ëª¨ë¸\"\"\"\n",
    "\n",
    "    category: Category = Field(\n",
    "        description=\"ì¿¼ë¦¬ë¥¼ ì¹´í…Œê³ ë¦¬í™” í•˜ì„¸ìš”. DOCUMENT(ë¬¸ì„œ ê´€ë ¨ ì§ˆë¬¸) / GENERAL(ì¼ë°˜ì ì¸ ì§ˆë¬¸) / GREETING(ì¸ì‚¬) ì¤‘ì— í•˜ë‚˜ë¡œ êµ¬ë¶„í•˜ì„¸ìš”.\"\n",
    "    )\n",
    "    reasoning: str = Field(description=\"ì™œ ì´ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí–ˆëŠ”ì§€ ì„¤ëª…í•˜ì„¸ìš”.\")\n",
    "\n",
    "\n",
    "# Define the state structure\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    context: List[str]\n",
    "    category: Optional[str]\n",
    "\n",
    "\n",
    "# Router function for categorizing queries\n",
    "def router(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ëŠ” ë¼ìš°í„°\"\"\"\n",
    "    print(\"ì¿¼ë¦¬ ë¶„ë¥˜ ì¤‘...\")\n",
    "\n",
    "    # Get the most recent user message\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "\n",
    "    # Create the router input\n",
    "    router_input = f\"\"\"\n",
    "    ë‹¤ìŒ ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ê³  ì¹´í…Œê³ ë¦¬ë¥¼ ê²°ì •í•˜ì„¸ìš”.\n",
    "    ì¹´í…Œê³ ë¦¬:\n",
    "    - document: ë¬¸ì„œ ë‚´ìš©ì— ê´€í•œ ì§ˆë¬¸ (ì˜ˆ: \"ì•„ì£¼ëŒ€í•™êµì— ëŒ€í•´ ì•Œë ¤ì¤˜\", \"ì´ ë¬¸ì„œì—ì„œ ì¤‘ìš”í•œ ë‚´ìš©ì€?\")\n",
    "    - general: ì¼ë°˜ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ, ë¬¸ì„œì™€ ê´€ë ¨ì´ ì—†ìŒ (ì˜ˆ: \"ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?\", \"íŒŒì´ì¬ì´ë€?\")\n",
    "    - greeting: ì¸ì‚¬ë§ (ì˜ˆ: \"ì•ˆë…•\", \"ë°˜ê°€ì›Œ\", \"ë­í•´?\")\n",
    "    \n",
    "    ì¿¼ë¦¬: {user_message}\n",
    "    \"\"\"\n",
    "\n",
    "    # Get LLM\n",
    "    llm = rag_components[\"llm\"]\n",
    "\n",
    "    # Structured output with the classification model\n",
    "    structured_llm = llm.with_structured_output(QueryClassification)\n",
    "\n",
    "    # Get classification\n",
    "    classification = structured_llm.invoke(router_input)\n",
    "    category = classification.category.value\n",
    "    print(f\"ë¶„ë¥˜ ê²°ê³¼: {category} (ì´ìœ : {classification.reasoning})\")\n",
    "\n",
    "    return {\"category\": category}\n",
    "\n",
    "\n",
    "# Conditional routing function\n",
    "def route_by_category(state: State) -> Literal[\"document_qa\", \"general_qa\", \"greeting\"]:\n",
    "    \"\"\"ì¹´í…Œê³ ë¦¬ì— ê¸°ë°˜í•˜ì—¬ ë‹¤ìŒ ë…¸ë“œë¥¼ ê²°ì •\"\"\"\n",
    "    category = state.get(\"category\", \"\").lower()\n",
    "\n",
    "    if category == \"document\":\n",
    "        return \"document_qa\"\n",
    "    elif category == \"general\":\n",
    "        return \"general_qa\"\n",
    "    elif category == \"greeting\":\n",
    "        return \"greeting\"\n",
    "    else:\n",
    "        # ê¸°ë³¸ê°’ì€ ì¼ë°˜ ì§ˆì˜ì‘ë‹µ\n",
    "        return \"general_qa\"\n",
    "\n",
    "\n",
    "# Define LangGraph nodes\n",
    "def retrieve_documents(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš© ê²€ìƒ‰\"\"\"\n",
    "    print(\"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...\")\n",
    "\n",
    "    # Get the most recent user message\n",
    "    user_message = state[\"messages\"][-1]\n",
    "\n",
    "    # Retrieve documents\n",
    "    retriever = rag_components[\"retriever\"]\n",
    "    docs = retriever.invoke(user_message.content)\n",
    "\n",
    "    # Format documents\n",
    "    formatted_docs = format_docs(docs)\n",
    "    print(f\"ê²€ìƒ‰ ì™„ë£Œ: {len(docs)} ë¬¸ì„œ ì°¾ìŒ\")\n",
    "\n",
    "    # Return updated state\n",
    "    return {\"context\": [formatted_docs]}\n",
    "\n",
    "\n",
    "def document_qa(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ\"\"\"\n",
    "    print(\"ë¬¸ì„œ ê¸°ë°˜ ì‘ë‹µ ìƒì„± ì¤‘...\")\n",
    "\n",
    "    # Format conversation history\n",
    "    history = state[\"messages\"][:-1]  # All messages except the current question\n",
    "\n",
    "    # Create formatted history for prompt context\n",
    "    formatted_history = \"\"\n",
    "    if history:\n",
    "        for msg in history:\n",
    "            role = \"ì‚¬ìš©ì\" if isinstance(msg, HumanMessage) else \"AI\"\n",
    "            formatted_history += f\"{role}: {msg.content}\\n\"\n",
    "    else:\n",
    "        formatted_history = \"ì´ì „ ëŒ€í™” ì—†ìŒ\"\n",
    "\n",
    "    # Get current question and context\n",
    "    question = state[\"messages\"][-1].content\n",
    "    context = state[\"context\"][0] if state[\"context\"] else \"ë¬¸ì„œ ì •ë³´ ì—†ìŒ\"\n",
    "\n",
    "    # Create prompt\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"ë„ˆëŠ” ì¹œì ˆí•œ í•œêµ­ì–´ AI ë¹„ì„œì•¼. \n",
    "            ì œê³µëœ ë¬¸ì„œ ë‚´ìš©(context)ê³¼ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µí•´.\n",
    "            ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ëŒ€ë‹µí•˜ê³ , ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ëŒ€ë‹µí•˜ì§€ ë§ê³  ëª¨ë¥¸ë‹¤ê³  í•´.\n",
    "            \n",
    "            ì°¸ê³  ë¬¸ì„œ:\n",
    "            {context}\n",
    "            \n",
    "            ì´ì „ ëŒ€í™”:\n",
    "            {chat_history}\n",
    "            \"\"\",\n",
    "            ),\n",
    "            (\"user\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Invoke the LLM\n",
    "    llm = rag_components[\"llm\"]\n",
    "\n",
    "    # Create formatted message for LLM\n",
    "    formatted_message = prompt.format_messages(\n",
    "        context=context, chat_history=formatted_history, question=question\n",
    "    )\n",
    "\n",
    "    # Get response from LLM\n",
    "    ai_response = llm.invoke(formatted_message)\n",
    "    response_content = ai_response.content\n",
    "\n",
    "    print(\"ë¬¸ì„œ ê¸°ë°˜ ì‘ë‹µ ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "    # Return the assistant message\n",
    "    return {\"messages\": [AIMessage(content=response_content)]}\n",
    "\n",
    "\n",
    "def general_qa(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"ì¼ë°˜ ì§ˆì˜ì‘ë‹µ\"\"\"\n",
    "    print(\"ì¼ë°˜ ì‘ë‹µ ìƒì„± ì¤‘...\")\n",
    "\n",
    "    # Get current question\n",
    "    question = state[\"messages\"][-1].content\n",
    "\n",
    "    # Create prompt\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"ë„ˆëŠ” ì¹œì ˆí•œ í•œêµ­ì–´ AI ë¹„ì„œì•¼. \n",
    "            ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´. í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•´.\n",
    "            \"\"\",\n",
    "            ),\n",
    "            (\"user\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Invoke the LLM\n",
    "    llm = rag_components[\"llm\"]\n",
    "\n",
    "    # Create formatted message for LLM\n",
    "    formatted_message = prompt.format_messages(question=question)\n",
    "\n",
    "    # Get response from LLM\n",
    "    ai_response = llm.invoke(formatted_message)\n",
    "    response_content = ai_response.content\n",
    "\n",
    "    print(\"ì¼ë°˜ ì‘ë‹µ ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "    # Return the assistant message\n",
    "    return {\"messages\": [AIMessage(content=response_content)]}\n",
    "\n",
    "\n",
    "def greeting(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"ì¸ì‚¬ë§ì— ì‘ë‹µ\"\"\"\n",
    "    print(\"ì¸ì‚¬ ì‘ë‹µ ìƒì„± ì¤‘...\")\n",
    "\n",
    "    # Get user message\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "\n",
    "    # Create prompt\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"ë„ˆëŠ” ì¹œì ˆí•œ í•œêµ­ì–´ AI ë¹„ì„œì•¼.\n",
    "            ì‚¬ìš©ìì˜ ì¸ì‚¬ì— ì¹œê·¼í•˜ê³  ë”°ëœ»í•˜ê²Œ ì‘ë‹µí•´. ê°„ê²°í•˜ê²Œ í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•´.\n",
    "            \"\"\",\n",
    "            ),\n",
    "            (\"user\", \"{greeting}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Invoke the LLM\n",
    "    llm = rag_components[\"llm\"]\n",
    "\n",
    "    # Create formatted message for LLM\n",
    "    formatted_message = prompt.format_messages(greeting=user_message)\n",
    "\n",
    "    # Get response from LLM\n",
    "    ai_response = llm.invoke(formatted_message)\n",
    "    response_content = ai_response.content\n",
    "\n",
    "    print(\"ì¸ì‚¬ ì‘ë‹µ ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "    # Return the assistant message\n",
    "    return {\"messages\": [AIMessage(content=response_content)]}\n",
    "\n",
    "\n",
    "# Run the graph with a given user input\n",
    "def run_graph(user_input: str):\n",
    "    \"\"\"Run the graph with a user input and return the response\"\"\"\n",
    "    # Create initial state with the user message\n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=user_input)],\n",
    "        \"context\": [],\n",
    "        \"category\": None,\n",
    "    }\n",
    "\n",
    "    # Run the graph and get the final state\n",
    "    result = graph.invoke(initial_state)\n",
    "\n",
    "    # Extract the AI response\n",
    "    if \"messages\" in result and len(result[\"messages\"]) > 1:\n",
    "        # Get the assistant message (should be the last one)\n",
    "        ai_msg = result[\"messages\"][-1]\n",
    "        if isinstance(ai_msg, AIMessage):\n",
    "            return ai_msg.content\n",
    "\n",
    "    return \"ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "\n",
    "# Interactive chat interface\n",
    "def interactive_chat():\n",
    "    \"\"\"Interactive chat interface for the RAG system\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ¤– LangGraph ë¼ìš°íŒ… RAG ì±—ë´‡ ì‹œì‘\")\n",
    "    print(\"ğŸ’¡ 'ì¢…ë£Œ' ì…ë ¥ ì‹œ ëŒ€í™”ë¥¼ ëëƒ…ë‹ˆë‹¤.\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    chat_history = []  # Track conversation for context\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"\\nğŸ™‹ ì‚¬ìš©ì: \").strip()\n",
    "\n",
    "                if not user_input:\n",
    "                    print(\"â— ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\")\n",
    "                    continue\n",
    "\n",
    "                if user_input.lower() == \"ì¢…ë£Œ\":\n",
    "                    print(\"ğŸ‘‹ ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤!\")\n",
    "                    break\n",
    "\n",
    "                # Add to history\n",
    "                chat_history.append((\"ì‚¬ìš©ì\", user_input))\n",
    "\n",
    "                # Get response\n",
    "                print(\"ğŸ¤– AI: \", end=\"\", flush=True)\n",
    "                response = run_graph(user_input)\n",
    "                print(response)\n",
    "\n",
    "                # Add to history\n",
    "                chat_history.append((\"AI\", response))\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤!\")\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "# Global variable for RAG components\n",
    "rag_components = None\n",
    "graph = None\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Initialize RAG components\n",
    "        print(\"RAG ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì¤‘...\")\n",
    "        rag_components = initialize_rag_components()\n",
    "\n",
    "        # Build the LangGraph\n",
    "        print(\"LangGraph êµ¬ì„± ì¤‘...\")\n",
    "        graph_builder = StateGraph(State)\n",
    "\n",
    "        # Add nodes\n",
    "        graph_builder.add_node(\"router\", router)\n",
    "        graph_builder.add_node(\"retrieve\", retrieve_documents)\n",
    "        graph_builder.add_node(\"document_qa\", document_qa)\n",
    "        graph_builder.add_node(\"general_qa\", general_qa)\n",
    "        graph_builder.add_node(\"greeting\", greeting)\n",
    "\n",
    "        # Add edges\n",
    "        graph_builder.add_edge(START, \"router\")\n",
    "\n",
    "        # Add conditional edges based on the category\n",
    "        graph_builder.add_conditional_edges(\n",
    "            \"router\",\n",
    "            route_by_category,\n",
    "            {\n",
    "                \"document_qa\": \"retrieve\",\n",
    "                \"general_qa\": \"general_qa\",\n",
    "                \"greeting\": \"greeting\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # Connect retrieve to document_qa\n",
    "        graph_builder.add_edge(\"retrieve\", \"document_qa\")\n",
    "\n",
    "        # Connect all output nodes to END\n",
    "        graph_builder.add_edge(\"document_qa\", END)\n",
    "        graph_builder.add_edge(\"general_qa\", END)\n",
    "        graph_builder.add_edge(\"greeting\", END)\n",
    "\n",
    "        # Compile the graph\n",
    "        graph = graph_builder.compile()\n",
    "        print(\"LangGraph êµ¬ì„± ì™„ë£Œ!\")\n",
    "\n",
    "        # Try to visualize the graph if possible\n",
    "        try:\n",
    "            print(\"ê·¸ë˜í”„ ì‹œê°í™” ì‹œë„ ì¤‘...\")\n",
    "            # This will only work in environments that support visualization\n",
    "            from IPython.display import Image, display\n",
    "\n",
    "            display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "            print(\"ê·¸ë˜í”„ ì‹œê°í™” ì™„ë£Œ!\")\n",
    "        except Exception as e:\n",
    "            print(f\"ê·¸ë˜í”„ ì‹œê°í™” ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "        # Start interactive chat\n",
    "        interactive_chat()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5297cce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
