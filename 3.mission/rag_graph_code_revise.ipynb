{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bcab1d9",
   "metadata": {},
   "source": [
    "ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed375125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baegjonghun/.pyenv/versions/3.11.10/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Annotated, Optional, Literal, List, Dict, Any\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "import enum\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import uuid\n",
    "\n",
    "# LangChain ë° ê¸°íƒ€ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "\n",
    "# LangGraph ê´€ë ¨ ì„í¬íŠ¸\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# ê¸°ì¡´ RAG ì½”ë“œì—ì„œ ê°€ì ¸ì˜¨ ì»´í¬ë„ŒíŠ¸\n",
    "from langchain_upstage import UpstageDocumentParseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ë­í“¨ì¦ˆ\n",
    "from langfuse import Langfuse, get_client\n",
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# ìŠ¤íŠ¸ë¦¬ë°\n",
    "import asyncio\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# ì„œì¹˜ íˆ´ travily\n",
    "import getpass\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "import json\n",
    "from langgraph.prebuilt import tools_condition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a8265c",
   "metadata": {},
   "source": [
    "ì„¤ì • ë° ì´ˆê¸°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edac3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .env íŒŒì¼ ë¡œë“œ (í•„ìš”í•œ ê²½ìš°)\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4087a03a",
   "metadata": {},
   "source": [
    "ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32285bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format documents function\n",
    "def format_docs(docs):\n",
    "    \"\"\"Format documents into a single string\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# 1. initialize_rag_components í•¨ìˆ˜ ìˆ˜ì • - Sonnet 4ë§Œ ì‚¬ìš©\n",
    "def initialize_rag_components(file_path: str = \"./test_modified.pdf\"):\n",
    "    \"\"\"Initialize all components for RAG\"\"\"\n",
    "    print(\"ë¬¸ì„œ ë¡œë”© ì¤‘...\")\n",
    "\n",
    "    # Document loading\n",
    "    loader = UpstageDocumentParseLoader(\n",
    "        file_path,\n",
    "        split=\"page\",\n",
    "        output_format=\"markdown\",\n",
    "        ocr=\"auto\",\n",
    "        coordinates=True,\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    print(f\"ë¬¸ì„œ ë¡œë”© ì™„ë£Œ: {len(docs)} í˜ì´ì§€\")\n",
    "\n",
    "    # Document chunking\n",
    "    print(\"ë¬¸ì„œ ì²­í‚¹ ì¤‘...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
    "    docs_splitter = splitter.split_documents(docs)\n",
    "    print(f\"ì²­í‚¹ ì™„ë£Œ: {len(docs_splitter)} ì²­í¬\")\n",
    "\n",
    "    # Embedding model\n",
    "    print(\"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "    device = \"cpu\"  # ê¸°ë³¸ê°’ìœ¼ë¡œ CPU ì‚¬ìš©\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "        elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "            device = \"mps\"\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "    print(f\"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "    hf_embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"intfloat/multilingual-e5-large-instruct\",\n",
    "        model_kwargs={\"device\": device},\n",
    "        encode_kwargs={\"normalize_embeddings\": True},\n",
    "    )\n",
    "\n",
    "    # Vector store\n",
    "    print(\"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘...\")\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents=docs_splitter,\n",
    "        embedding=hf_embeddings,\n",
    "    )\n",
    "    print(\"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "    # Retriever\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 5},\n",
    "    )\n",
    "\n",
    "    # Sonnet 4 LLM - ëª¨ë“  ì‘ì—…ìš©\n",
    "    print(\"ğŸš€ Sonnet 4 LLM ì´ˆê¸°í™” ì¤‘...\")\n",
    "    sonnet_llm = ChatAnthropic(\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        temperature=0,\n",
    "        streaming=True,\n",
    "    )\n",
    "\n",
    "    #ì›¹ ê²€ìƒ‰ íˆ´ê³¼ ë°”ì¸ë”©\n",
    "    tools = [tavily_web_search]\n",
    "    sonnet_llm_with_tools = sonnet_llm.bind_tools(tools)\n",
    "    \n",
    "    print(\"âœ… Sonnet 4 LLM ì´ˆê¸°í™” ì™„ë£Œ!\")\n",
    "\n",
    "    return {\n",
    "        \"retriever\": retriever,\n",
    "        \"llm\": sonnet_llm,                    # ëª¨ë“  ì‘ì—…ìš© Sonnet 4\n",
    "        \"llm_with_tools\": sonnet_llm_with_tools,  # ì›¹ ê²€ìƒ‰ìš© Sonnet 4\n",
    "        \"tools\": tools\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e1f53b",
   "metadata": {},
   "source": [
    "ëª¨ë¸ í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d55733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì›¹ ê²€ìƒ‰ íˆ´ í•¨ìˆ˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# Define categories for query classification\n",
    "class Category(enum.Enum):\n",
    "    DOCUMENT = \"document\"  # ë¬¸ì„œ ê´€ë ¨ ì§ˆë¬¸\n",
    "    GENERAL =\"general\"\n",
    "\n",
    "# Pydantic model for structured output\n",
    "class QueryClassification(BaseModel):\n",
    "    \"\"\"ì‚¬ìš©ì ì¿¼ë¦¬ ë¶„ë¥˜ ëª¨ë¸\"\"\"\n",
    "\n",
    "    category: Category = Field(\n",
    "        description=\"ì¿¼ë¦¬ë¥¼ ì¹´í…Œê³ ë¦¬í™” í•˜ì„¸ìš”. DOCUMENT(ë¬¸ì„œ ê´€ë ¨ ì§ˆë¬¸) / GENERAL(ì¼ë°˜ì ì¸ ì§ˆë¬¸) ì¤‘ì— í•˜ë‚˜ë¡œ êµ¬ë¶„í•˜ì„¸ìš”.\"\n",
    "    )\n",
    "    reasoning: str = Field(description=\"ì™œ ì´ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí–ˆëŠ”ì§€ ì„¤ëª…í•˜ì„¸ìš”.\")\n",
    "\n",
    "\n",
    "# Define the state structure\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    context: List[str]\n",
    "    category: Optional[str]\n",
    "    transformed_query: Optional[str]  # ğŸ†• ë³€í™˜ëœ ì¿¼ë¦¬ ì €ì¥\n",
    "\n",
    "\n",
    "# 2. setup_tool í•¨ìˆ˜ ë‹¨ìˆœí™”\n",
    "def setup_tool():\n",
    "    \"\"\"ì›¹ ê²€ìƒ‰ íˆ´ ë…¸ë“œ ìƒì„±\"\"\"\n",
    "    global rag_components\n",
    "    \n",
    "    # ToolNode ìƒì„± (ì´ë¯¸ rag_componentsì— toolsê°€ ìˆìŒ)\n",
    "    tool_node = ToolNode(rag_components[\"tools\"])\n",
    "    \n",
    "    print(\"âœ… ì›¹ ê²€ìƒ‰ íˆ´ ì„¤ì • ì™„ë£Œ!\")\n",
    "    return tool_node\n",
    "\n",
    "# 3. router í•¨ìˆ˜ ìˆ˜ì • - Sonnet 4 ì‚¬ìš©\n",
    "def router(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ëŠ” ë¼ìš°í„° - Sonnet 4 ì‚¬ìš©\"\"\"\n",
    "    print(\"ğŸš€ ì¿¼ë¦¬ ë¶„ë¥˜ ì¤‘ (Sonnet 4)...\")\n",
    "\n",
    "    # Get the most recent user message\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "\n",
    "    # Create the router input\n",
    "    router_input = f\"\"\"\n",
    "    ë‹¤ìŒ ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ê³  ì¹´í…Œê³ ë¦¬ë¥¼ ê²°ì •í•˜ì„¸ìš”.\n",
    "    ì¹´í…Œê³ ë¦¬:\n",
    "    - document: ë¬¸ì„œ ë‚´ìš©ì— ê´€í•œ ì§ˆë¬¸ (ì˜ˆ: \"ì•„ì£¼ëŒ€í•™êµì— ëŒ€í•´ ì•Œë ¤ì¤˜\", \"ì´ ë¬¸ì„œì—ì„œ ì¤‘ìš”í•œ ë‚´ìš©ì€?\")\n",
    "    - general: ì¼ë°˜ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ, ë¬¸ì„œì™€ ê´€ë ¨ì´ ì—†ìŒ (ì˜ˆ: \"ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?\", \"íŒŒì´ì¬ì´ë€?\")\n",
    "    \n",
    "    ì¿¼ë¦¬: {user_message}\n",
    "    \"\"\"\n",
    "\n",
    "    # Sonnet 4 LLM ì‚¬ìš©\n",
    "    llm = rag_components[\"llm\"]\n",
    "\n",
    "    # Structured output with the classification model\n",
    "    structured_llm = llm.with_structured_output(QueryClassification)\n",
    "\n",
    "    # Get classification\n",
    "    classification = structured_llm.invoke(router_input)\n",
    "\n",
    "    category = classification.category.value\n",
    "    print(f\"ë¶„ë¥˜ ê²°ê³¼: {category} (ì´ìœ : {classification.reasoning}) - Sonnet 4\")\n",
    "\n",
    "    return {\"category\": category}\n",
    "\n",
    "\n",
    "# Conditional routing function\n",
    "def route_by_category(state: State) -> Literal[\"document_qa\", \"general_qa\"]:\n",
    "    \"\"\"ì¹´í…Œê³ ë¦¬ì— ê¸°ë°˜í•˜ì—¬ ë‹¤ìŒ ë…¸ë“œë¥¼ ê²°ì •\"\"\"\n",
    "    category = state.get(\"category\", \"\").lower()\n",
    "\n",
    "    if category == \"document\":\n",
    "        return \"document_qa\"\n",
    "    elif category == \"general\":\n",
    "        return \"general_qa\"\n",
    "    else:\n",
    "        # ê¸°ë³¸ê°’ì€ ì¼ë°˜ ì§ˆì˜ì‘ë‹µ\n",
    "        return \"general_qa\"\n",
    "\n",
    "\n",
    "def query_transform(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë¬¸ì„œ ê²€ìƒ‰ì— ìµœì í™”ëœ í˜•íƒœë¡œ ë³€í™˜\"\"\"\n",
    "    print(\"ğŸ”„ ì¿¼ë¦¬ ë³€í™˜ ì¤‘ (Sonnet 4)...\")\n",
    "    \n",
    "    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ\n",
    "    user_message = None\n",
    "    for msg in reversed(state[\"messages\"]):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            user_message = msg.content\n",
    "            break\n",
    "    \n",
    "    if not user_message:\n",
    "        return {\"transformed_query\": \"\"}\n",
    "    \n",
    "    # ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±\n",
    "    history_messages = state[\"messages\"][:-1]\n",
    "    formatted_history = \"\"\n",
    "    for msg in history_messages:\n",
    "        role = \"ì‚¬ìš©ì\" if isinstance(msg, HumanMessage) else \"AI\"\n",
    "        formatted_history += f\"{role}: {msg.content}\\n\"\n",
    "    \n",
    "    # Query Transform í”„ë¡¬í”„íŠ¸\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"ë„ˆëŠ” ë¬¸ì„œ ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” ì „ë¬¸ê°€ì•¼.\n",
    "        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•´ì„œ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ë” ì˜ ì°¾ì„ ìˆ˜ ìˆë„ë¡ ì¿¼ë¦¬ë¥¼ ë³€í™˜í•´.\n",
    "        \n",
    "        ## ì¿¼ë¦¬ ë³€í™˜ ê·œì¹™:\n",
    "        1. **í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ**: ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°í•˜ê³  í•µì‹¬ ë‹¨ì–´ë§Œ ë‚¨ê¸°ê¸°\n",
    "        2. **ë™ì˜ì–´ í™•ì¥**: ê´€ë ¨ ìš©ì–´ë“¤ ì¶”ê°€ (ì˜ˆ: \"ëŒ€í•™êµ\" â†’ \"ëŒ€í•™êµ, ëŒ€í•™, í•™êµ\")\n",
    "        3. **ê²€ìƒ‰ ìµœì í™”**: ë¬¸ì„œì—ì„œ ì°¾ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜\n",
    "        4. **ì»¨í…ìŠ¤íŠ¸ ë°˜ì˜**: ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•œ ì¿¼ë¦¬ í™•ì¥\n",
    "        5. **êµ¬ì²´í™”**: ëª¨í˜¸í•œ í‘œí˜„ì„ êµ¬ì²´ì ìœ¼ë¡œ ë³€í™˜\n",
    "        \n",
    "        ## ë³€í™˜ ì˜ˆì‹œ:\n",
    "        - \"ì•„ì£¼ëŒ€ì— ëŒ€í•´ ì•Œë ¤ì¤˜\" â†’ \"ì•„ì£¼ëŒ€í•™êµ ëŒ€í•™ ì •ë³´ ê°œìš” ì†Œê°œ\"\n",
    "        - \"ì…í•™ ì¡°ê±´ì´ ë­ì•¼?\" â†’ \"ì…í•™ ì¡°ê±´ ì§€ì› ìê²© ìš”ê±´ ëª¨ì§‘\"\n",
    "        - \"í•™ê³¼ëŠ” ì–´ë–¤ê²Œ ìˆì–´?\" â†’ \"í•™ê³¼ ì „ê³µ ë‹¨ê³¼ëŒ€í•™ ê³„ì—´ ì „ê³µë¶„ì•¼\"\n",
    "        \n",
    "        ## ì‘ë‹µ í˜•ì‹:\n",
    "        ì¿¼ë¦¬ê°€ ì–´ë–»ê²Œ ë³€ê²½ë˜ì—ˆëŠ”ì§€ ì•Œë ¤ì¤˜\n",
    "        \n",
    "        ì´ì „ ëŒ€í™”:\n",
    "        {chat_history}\n",
    "        \"\"\"),\n",
    "        (\"user\", \"ì›ë³¸ ì¿¼ë¦¬: {original_query}\")\n",
    "    ])\n",
    "    \n",
    "    # Sonnet 4 ì‚¬ìš©\n",
    "    llm = rag_components[\"llm\"]\n",
    "    \n",
    "    formatted_message = prompt.format_messages(\n",
    "        chat_history=formatted_history,\n",
    "        original_query=user_message\n",
    "    )\n",
    "    \n",
    "    ai_response = llm.invoke(formatted_message)\n",
    "    transformed_query = ai_response.content.strip()\n",
    "    \n",
    "    print(f\"âœ… ì¿¼ë¦¬ ë³€í™˜ ì™„ë£Œ: '{user_message}' â†’ '{transformed_query}'\")\n",
    "    \n",
    "    return {\"transformed_query\": transformed_query}\n",
    "\n",
    "# Define LangGraph nodes\n",
    "def retrieve_documents(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš© ê²€ìƒ‰\"\"\"\n",
    "    print(\"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...\")\n",
    "\n",
    "    # Get the most recent user message\n",
    "    user_message = state[\"messages\"][-1]\n",
    "\n",
    "    # Retrieve documents\n",
    "    retriever = rag_components[\"retriever\"]\n",
    "    docs = retriever.invoke(user_message.content)\n",
    "\n",
    "    # Format documents\n",
    "    formatted_docs = format_docs(docs)\n",
    "    print(f\"ê²€ìƒ‰ ì™„ë£Œ: {len(docs)} ë¬¸ì„œ ì°¾ìŒ\")\n",
    "\n",
    "    # Return updated state\n",
    "    return {\"context\": [formatted_docs]}\n",
    "\n",
    "\n",
    "\n",
    "# 1. TavilySearch ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì˜¬ë°”ë¥´ê²Œ ìƒì„±\n",
    "tavily_search_tool = TavilySearch(\n",
    "    max_results=3,\n",
    "    topic=\"general\"\n",
    ")\n",
    "\n",
    "@tool\n",
    "def tavily_web_search(query: str) -> str:\n",
    "    \"\"\"ì›¹ì—ì„œ ìµœì‹  ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤. ë‰´ìŠ¤, ë‚ ì”¨, ì‹¤ì‹œê°„ ë°ì´í„° ë“±ì— ì‚¬ìš©í•˜ì„¸ìš”.\"\"\"\n",
    "    print(f\"ğŸŒ Tavily ì›¹ ê²€ìƒ‰ ì¤‘: {query}\")\n",
    "    \n",
    "    try:\n",
    "        # ì´ì œ tavily_search_toolì´ ì˜¬ë°”ë¥¸ TavilySearch ì¸ìŠ¤í„´ìŠ¤ì„\n",
    "        search_results = tavily_search_tool.invoke({\"query\": query})\n",
    "        print(f\"âœ… ê²€ìƒ‰ ê²°ê³¼ ë°›ìŒ: {type(search_results)}\")\n",
    "        \n",
    "        # ê²°ê³¼ í¬ë§·íŒ…\n",
    "        if isinstance(search_results, list):\n",
    "            if len(search_results) == 0:\n",
    "                return \"ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "                \n",
    "            formatted_results = []\n",
    "            for i, result in enumerate(search_results[:3]):  # ìƒìœ„ 3ê°œë§Œ\n",
    "                title = result.get('title', 'N/A')\n",
    "                content = result.get('content', 'N/A')\n",
    "                url = result.get('url', 'N/A')\n",
    "                \n",
    "                formatted_results.append(f\"\"\"\n",
    "ê²€ìƒ‰ ê²°ê³¼ {i+1}:\n",
    "ì œëª©: {title}\n",
    "ë‚´ìš©: {content[:300] if content != 'N/A' else 'N/A'}...\n",
    "ì¶œì²˜: {url}\n",
    "\"\"\")\n",
    "            return \"\\n\".join(formatted_results)\n",
    "        else:\n",
    "            return f\"ê²€ìƒ‰ ê²°ê³¼: {str(search_results)[:500]}\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = f\"ì›¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\"\n",
    "        print(f\"âŒ {error_msg}\")\n",
    "        return error_msg\n",
    "\n",
    "print(\"âœ… ì›¹ ê²€ìƒ‰ íˆ´ í•¨ìˆ˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ!\")\n",
    "\n",
    "# ì›¹ ê²€ìƒ‰ íˆ´ì„ ê¸°ì¡´ LLMì— ë°”ì¸ë”©\n",
    "# def bind_web_search_to_llm():\n",
    "#     \"\"\"ê¸°ì¡´ LLMì— ì›¹ ê²€ìƒ‰ íˆ´ì„ ë°”ì¸ë”©í•©ë‹ˆë‹¤.\"\"\"\n",
    "#     global rag_components\n",
    "\n",
    "#     # ì›¹ ê²€ìƒ‰ íˆ´ ë¦¬ìŠ¤íŠ¸\n",
    "#     tool = [tavily_web_search]\n",
    "\n",
    "#     # ê¸°ì¡´ LLMì— íˆ´ ë°”ì¸ë”©\n",
    "#     llm = rag_components[\"llm\"]\n",
    "#     llm_with_tools = llm.bind_tools(tool)\n",
    "\n",
    "#     # rag_components ì—…ë°ì´íŠ¸\n",
    "#     rag_components[\"llm_with_tools\"] = llm_with_tools\n",
    "#     rag_components[\"tool\"] = tool\n",
    "\n",
    "#     print(\"âœ… ê¸°ì¡´ LLMì— ì›¹ ê²€ìƒ‰ íˆ´ ë°”ì¸ë”© ì™„ë£Œ!\")\n",
    "#     return llm_with_tools #ì—¬ê¸°ì„œ rutune ì–´ë–»ê²Œ ì‚¬ìš©ë˜ë‚˜???????????????????????????????????????????????\n",
    "\n",
    "# def setup_dual_model_system():\n",
    "#     \"\"\"íˆ´ ì‚¬ìš©ì‹œì™€ ì¼ë°˜ ì‘ë‹µì‹œ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •\"\"\"\n",
    "#     print(\"ğŸ”§ ë“€ì–¼ ëª¨ë¸ ì‹œìŠ¤í…œ ì„¤ì • ì¤‘...\")\n",
    "    \n",
    "#     # 1. ê¸°ë³¸ ëª¨ë¸ (Haiku 3) - ë¹ ë¥¸ ì‘ë‹µìš©\n",
    "#     base_llm = ChatAnthropic(\n",
    "#         model=\"claude-3-haiku-20240307\",\n",
    "#         temperature=0,\n",
    "#         streaming=True,\n",
    "#     )\n",
    "    \n",
    "#     # 2. ê³ ì„±ëŠ¥ ëª¨ë¸ (Sonnet 4) - íˆ´ ì‚¬ìš©ì‹œ\n",
    "#     advanced_llm = ChatAnthropic(\n",
    "#         model=\"claude-sonnet-4-20250514\",  # ğŸš€ Sonnet 4 ì‚¬ìš©\n",
    "#         temperature=0,\n",
    "#         streaming=True,\n",
    "#     )\n",
    "    \n",
    "#     # 3. íˆ´ì´ ë°”ì¸ë”©ëœ ê³ ì„±ëŠ¥ ëª¨ë¸\n",
    "#     tools = [tavily_web_search]\n",
    "#     advanced_llm_with_tools = advanced_llm.bind_tools(tools)\n",
    "    \n",
    "#     # 4. ì»´í¬ë„ŒíŠ¸ ë°˜í™˜\n",
    "#     components = {\n",
    "#         \"llm\": base_llm,                           # ê¸°ë³¸ ëª¨ë¸ (Haiku 3)\n",
    "#         \"advanced_llm\": advanced_llm,              # ê³ ì„±ëŠ¥ ëª¨ë¸ (Sonnet 4)\n",
    "#         \"llm_with_tools\": advanced_llm_with_tools, # íˆ´ ë°”ì¸ë”©ëœ ê³ ì„±ëŠ¥ ëª¨ë¸\n",
    "#         \"tools\": tools\n",
    "#     }\n",
    "    \n",
    "#     print(\"âœ… ë“€ì–¼ ëª¨ë¸ ì‹œìŠ¤í…œ ì„¤ì • ì™„ë£Œ!\")\n",
    "#     print(\"ğŸ“Š ê¸°ë³¸ ì‘ë‹µ: Claude Haiku 3 (ë¹ ë¦„)\")\n",
    "#     print(\"ğŸ§  íˆ´ ì‚¬ìš©ì‹œ: Claude Sonnet 4 (ê³ ì„±ëŠ¥)\")\n",
    "    \n",
    "#     return components\n",
    "\n",
    " #4. document_qa í•¨ìˆ˜ ìˆ˜ì • - Sonnet 4 ì‚¬ìš©\n",
    "def document_qa(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ - Sonnet 4 ì‚¬ìš©\"\"\"\n",
    "    print(\"ğŸš€ ë¬¸ì„œ ê¸°ë°˜ ì‘ë‹µ ìƒì„± ì¤‘ (Sonnet 4)...\")\n",
    "    context = state[\"context\"][0] if state[\"context\"] else \"ë¬¸ì„œ ì •ë³´ ì—†ìŒ\"\n",
    "    \n",
    "    # Get user message\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "\n",
    "    # ì´ì „ ëŒ€í™”ë“¤ì€ ë³„ë„ë¡œ íˆìŠ¤í† ë¦¬ êµ¬ì„±\n",
    "    history_messages = state[\"messages\"][:-1]\n",
    "    formatted_history = \"\"\n",
    "    for msg in history_messages:\n",
    "        role = \"ì‚¬ìš©ì\" if isinstance(msg, HumanMessage) else \"AI\"\n",
    "        formatted_history += f\"{role}: {msg.content}\\n\"\n",
    "\n",
    "    # Create prompt\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"ë„ˆëŠ” ì¹œì ˆí•œ í•œêµ­ì–´ AI ë¹„ì„œì•¼. \n",
    "        ì œê³µëœ ë¬¸ì„œ ë‚´ìš©(context)ê³¼ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µí•´.\n",
    "        ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ëŒ€ë‹µí•˜ê³ , ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ëŒ€ë‹µí•˜ì§€ ë§ê³  ëª¨ë¥¸ë‹¤ê³  í•´.\n",
    "        \n",
    "        ì°¸ê³  ë¬¸ì„œ:\n",
    "        {context}\n",
    "        \n",
    "        ì´ì „ ëŒ€í™”:\n",
    "        {chat_history}\n",
    "        \"\"\"),\n",
    "        (\"user\", \"{user_input}\"),\n",
    "    ])\n",
    "\n",
    "    # Sonnet 4 LLM ì‚¬ìš©\n",
    "    llm = rag_components[\"llm\"]\n",
    "\n",
    "    # Create formatted message for LLM\n",
    "    formatted_message = prompt.format_messages(\n",
    "        context=context,\n",
    "        chat_history=formatted_history,\n",
    "        user_input=user_message,\n",
    "    )\n",
    "\n",
    "    # Get response from LLM\n",
    "    ai_response = llm.invoke(formatted_message)\n",
    "    response_content = ai_response.content\n",
    "    print(\"âœ… ë¬¸ì„œ ê¸°ë°˜ ì‘ë‹µ ìƒì„± ì™„ë£Œ (Sonnet 4)\")\n",
    "\n",
    "    # Return the assistant message\n",
    "    return {\"messages\": [AIMessage(content=response_content)]}\n",
    "\n",
    "# 5. general_qa í•¨ìˆ˜ ìˆ˜ì • - Sonnet 4 with Tools ì‚¬ìš©\n",
    "def general_qa(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"ì¼ë°˜ ì§ˆì˜ì‘ë‹µ - Sonnet 4 with Tools ì‚¬ìš©\"\"\"\n",
    "    print(\"ğŸš€ ì¼ë°˜ ì‘ë‹µ ìƒì„± ì¤‘ (Sonnet 4 + ì›¹ íˆ´)...\")\n",
    "\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "\n",
    "    # ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±\n",
    "    history_messages = state[\"messages\"][:-1]\n",
    "    formatted_history = \"\"\n",
    "    for msg in history_messages:\n",
    "        role = \"ì‚¬ìš©ì\" if isinstance(msg, HumanMessage) else \"AI\"\n",
    "        formatted_history += f\"{role}: {msg.content}\\\\n\"\n",
    "\n",
    "    # Sonnet 4 + íˆ´ ì‚¬ìš©\n",
    "    llm_with_tools = rag_components[\"llm_with_tools\"]\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"ë„ˆëŠ” ì¹œì ˆí•œ í•œêµ­ì–´ AI ë¹„ì„œì•¼. \n",
    "        ì‚¬ìš©ìì˜ ì¼ë°˜ì ì¸ ì§ˆë¬¸ì— ë‹µë³€í•´. \n",
    "        ë§Œì•½ ìµœì‹  ì •ë³´ë‚˜ ì‹¤ì‹œê°„ ë°ì´í„°ê°€ í•„ìš”í•˜ë‹¤ë©´ (ë‚ ì”¨, ë‰´ìŠ¤, ì£¼ì‹ ë“±), tavily_web_search íˆ´ì„ ì‚¬ìš©í•´ì„œ ì›¹ì—ì„œ ì •ë³´ë¥¼ ì°¾ì•„ì¤˜. \n",
    "## ReAct ë‹¨ê³„ë³„ ì²˜ë¦¬ ë°©ì‹:\n",
    "\n",
    "**1ë‹¨ê³„ - ì‚¬ê³  (Thought):** ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ì–´ë–¤ ì •ë³´ê°€ í•„ìš”í•œì§€ ìƒê°í•´\n",
    "- ì´ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ ë¬´ì—‡ì´ í•„ìš”í•œê°€?\n",
    "- ë‚´ê°€ ì´ë¯¸ ì•Œê³  ìˆëŠ” ì •ë³´ì¸ê°€, ì•„ë‹ˆë©´ ìµœì‹  ì •ë³´ê°€ í•„ìš”í•œê°€?\n",
    "- ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œê°€?\n",
    "\n",
    "**2ë‹¨ê³„ - í–‰ë™ (Action):** í•„ìš”í•˜ë‹¤ë©´ ì ì ˆí•œ ë„êµ¬ë¥¼ ì‚¬ìš©í•´\n",
    "- ìµœì‹  ì •ë³´ë‚˜ ì‹¤ì‹œê°„ ë°ì´í„°ê°€ í•„ìš”í•˜ë©´: tavily_web_search íˆ´ ì‚¬ìš©\n",
    "- ì¼ë°˜ ìƒì‹ìœ¼ë¡œ ë‹µë³€ ê°€ëŠ¥í•˜ë©´: ë°”ë¡œ ë‹µë³€\n",
    "\n",
    "**3ë‹¨ê³„ - ê´€ì°° (Observation):** ë„êµ¬ ì‚¬ìš© ê²°ê³¼ë¥¼ ë¶„ì„í•´\n",
    "- ê²€ìƒ‰ ê²°ê³¼ê°€ ì§ˆë¬¸ì— ì í•©í•œê°€?\n",
    "- ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•œê°€?\n",
    "\n",
    "**4ë‹¨ê³„ - ë‹µë³€ (Answer):** ìµœì¢… ë‹µë³€ì„ ì œê³µí•´\n",
    "\n",
    "## ë‹µë³€ í˜•ì‹:\n",
    "**ì‚¬ê³ :** [ì§ˆë¬¸ ë¶„ì„ ë° í•„ìš”í•œ ì •ë³´ íŒë‹¨]\n",
    "**í–‰ë™:** [ì·¨í•  í–‰ë™ - ì›¹ ê²€ìƒ‰ ë˜ëŠ” ì§ì ‘ ë‹µë³€]\n",
    "**ë‹µë³€:** [ì‚¬ìš©ìì—ê²Œ ì œê³µí•  ìµœì¢… ë‹µë³€]\n",
    "\n",
    "## ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš°:\n",
    "- ë‚ ì”¨, ë‰´ìŠ¤, ì£¼ì‹ ê°€ê²©\n",
    "- í˜„ì¬ ë‚ ì§œ, ì‹œê°„\n",
    "- ìµœì‹  ì‚¬ê±´, íŠ¸ë Œë“œ\n",
    "- ì‹¤ì‹œê°„ ë°ì´í„°\n",
    "\n",
    "## ì§ì ‘ ë‹µë³€ ê°€ëŠ¥í•œ ê²½ìš°:\n",
    "- ì¼ë°˜ ìƒì‹, ì—­ì‚¬ì  ì‚¬ì‹¤\n",
    "- ìˆ˜í•™ ê³„ì‚°, ì–¸ì–´ ë²ˆì—­\n",
    "- ê°œë… ì„¤ëª…, ì •ì˜\n",
    "- ê¸°ë³¸ì ì¸ ì¸ì‚¬ë§\n",
    "         \n",
    "\n",
    "## ì¼ë°˜ì ì¸ ìƒì‹ìœ¼ë¡œ ë‹µë³€ ê°€ëŠ¥í•œ ì§ˆë¬¸ì€ ë°”ë¡œ ë‹µë³€í•´.\n",
    "        \n",
    "        ì´ì „ ëŒ€í™”:\n",
    "        {chat_history}\n",
    "        \"\"\"),\n",
    "        (\"user\", \"{user_input}\"),\n",
    "    ])\n",
    "\n",
    "    formatted_message = prompt.format_messages(\n",
    "        chat_history=formatted_history,\n",
    "        user_input=user_message,\n",
    "    )\n",
    "\n",
    "    ai_response = llm_with_tools.invoke(formatted_message)\n",
    "    print(\"âœ… ì¼ë°˜ ì‘ë‹µ ìƒì„± ì™„ë£Œ (Sonnet 4)\")\n",
    "\n",
    "    return {\"messages\": [ai_response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7f07b3",
   "metadata": {},
   "source": [
    "ê·¸ë˜í”„ ë…¸ë“œ í•¨ìˆ˜ ë° ê·¸ë˜í”„ ë¹Œë“œ ë° ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144be721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í•¨ìˆ˜ë“¤ ì •ì˜ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# 1. ë¨¼ì € í•„ìš”í•œ ë³€ìˆ˜ë“¤ ì •ì˜\n",
    "conversation_history = []  # ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥\n",
    "\n",
    "# 2. setup_advanced_llm í•¨ìˆ˜ ì¶”ê°€ - Advanced LLM ì„¤ì •\n",
    "def setup_advanced_llm():\n",
    "    \"\"\"Advanced LLM (Sonnet 4) with tools ì„¤ì •\"\"\"\n",
    "    print(\"ğŸ§  Advanced LLM (Sonnet 4) ì„¤ì • ì¤‘...\")\n",
    "    \n",
    "    # Advanced LLM (Sonnet 4) - ì›¹ ê²€ìƒ‰ìš©\n",
    "    advanced_llm = ChatAnthropic(\n",
    "        model=\"claude-sonnet-4-20250514\",  # ğŸš€ Sonnet 4 ì‚¬ìš©\n",
    "        temperature=0,\n",
    "        streaming=True,\n",
    "    )\n",
    "    \n",
    "    # ì›¹ ê²€ìƒ‰ íˆ´ê³¼ ë°”ì¸ë”©\n",
    "    tools = [tavily_web_search]\n",
    "    advanced_llm_with_tools = advanced_llm.bind_tools(tools)\n",
    "    \n",
    "    print(\"âœ… Advanced LLM (Sonnet 4) ì„¤ì • ì™„ë£Œ!\")\n",
    "    return {\n",
    "        \"advanced_llm\": advanced_llm,\n",
    "        \"advanced_llm_with_tools\": advanced_llm_with_tools,\n",
    "        \"tools\": tools\n",
    "    }\n",
    "\n",
    "\n",
    "# 4. Run graph í•¨ìˆ˜\n",
    "def run_graph(user_input: str):\n",
    "    global conversation_history\n",
    "\n",
    "    # ìƒˆë¡œìš´ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€\n",
    "    user_message = HumanMessage(content=user_input)\n",
    "    conversation_history.append(user_message)\n",
    "\n",
    "    # Create initial state with the user message\n",
    "    initial_state = {\n",
    "        \"messages\": conversation_history,  # ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ëˆ„ì ëœ ë°°ì—´\n",
    "        \"context\": [],\n",
    "        \"category\": None,\n",
    "    }\n",
    "\n",
    "    # Run the graph and get the final state\n",
    "    result = graph.invoke(initial_state)\n",
    "\n",
    "    # Extract the AI response\n",
    "    if \"messages\" in result and len(result[\"messages\"]) > 1:\n",
    "        # Get the assistant message (should be the last one)\n",
    "        ai_msg = result[\"messages\"][-1]\n",
    "        if isinstance(ai_msg, AIMessage):\n",
    "            # AI ì‘ë‹µì„ conversation_historyì— ì¶”ê°€\n",
    "            conversation_history.append(ai_msg)\n",
    "            return ai_msg.content\n",
    "\n",
    "    return \"ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "# 5. Interactive chat í•¨ìˆ˜\n",
    "def interactive_chat():\n",
    "    \"\"\"Interactive chat interface for the RAG system\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ¤– LangGraph ë¼ìš°íŒ… RAG ì±—ë´‡ ì‹œì‘ (ë©€í‹°í„´ ëŒ€í™” ì§€ì›)\")\n",
    "    print(\"ğŸ’¡ 'ì¢…ë£Œ' ì…ë ¥ ì‹œ ëŒ€í™”ë¥¼ ëëƒ…ë‹ˆë‹¤.\")\n",
    "    print(\"ğŸ’¡ 'íˆìŠ¤í† ë¦¬' ì…ë ¥ ì‹œ í˜„ì¬ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"\\nğŸ™‹ ì‚¬ìš©ì: \").strip()\n",
    "\n",
    "                if not user_input:\n",
    "                    print(\"â— ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\")\n",
    "                    continue\n",
    "\n",
    "                if user_input.lower() == \"ì¢…ë£Œ\":\n",
    "                    print(\"ğŸ‘‹ ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤!\")\n",
    "                    break\n",
    "\n",
    "                # íˆìŠ¤í† ë¦¬ í™•ì¸ ëª…ë ¹ì–´ ì¶”ê°€\n",
    "                if user_input.lower() == \"íˆìŠ¤í† ë¦¬\":\n",
    "                    print(\"\\n=== í˜„ì¬ ëŒ€í™” íˆìŠ¤í† ë¦¬ ===\")\n",
    "                    for i, msg in enumerate(conversation_history):\n",
    "                        msg_type = \"ì‚¬ìš©ì\" if isinstance(msg, HumanMessage) else \"AI\"\n",
    "                        content = (\n",
    "                            msg.content[:100] + \"...\"\n",
    "                            if len(msg.content) > 100\n",
    "                            else msg.content\n",
    "                        )\n",
    "                        print(f\"{i+1}. {msg_type}: {content}\")\n",
    "                    print(f\"ì´ {len(conversation_history)}ê°œ ë©”ì‹œì§€\")\n",
    "                    print(\"=\" * 30)\n",
    "                    continue\n",
    "\n",
    "                # Get response\n",
    "                print(\"ğŸ¤– AI: \", end=\"\", flush=True)\n",
    "                response = run_graph(user_input)\n",
    "                print(response)\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nğŸ‘‹ ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤!\")\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"âœ… í•¨ìˆ˜ë“¤ ì •ì˜ ì™„ë£Œ!\")\n",
    "\n",
    "\n",
    "# Global variables\n",
    "rag_components = None #ragë¡œ ì´ë¯¸ ë§Œë“¤ì–´ì§„ êµ¬ì„±ìš”ì†Œë“¤ì´ ë‹´ê¸´ ë”•ì…”ë„ˆë¦¬êµ¬ë‚˜\n",
    "graph = None\n",
    "\n",
    "\n",
    "# 6. main í•¨ìˆ˜ ìˆ˜ì •\n",
    "def main():\n",
    "    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n",
    "    global rag_components, graph\n",
    "    \n",
    "    try:\n",
    "        # 1. RAG ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” (Sonnet 4ë§Œ)\n",
    "        print(\"RAG ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì¤‘...\")\n",
    "        rag_components = initialize_rag_components()\n",
    "        \n",
    "        # 2. ì›¹ íˆ´ ì„¤ì •\n",
    "        print(\"ì›¹ ê²€ìƒ‰ íˆ´ ì„¤ì • ì¤‘...\")\n",
    "        tool_node = setup_tool()\n",
    "        \n",
    "        # 3. ê·¸ë˜í”„ ë¹Œë“œ\n",
    "        print(\"ğŸš€ ëª¨ë“  ì‘ì—…ì— Sonnet 4ë¥¼ ì‚¬ìš©í•˜ëŠ” ê·¸ë˜í”„ êµ¬ì„± ì¤‘...\")\n",
    "        \n",
    "        graph_builder = StateGraph(State)\n",
    "        \n",
    "        # ë…¸ë“œë“¤ ì¶”ê°€\n",
    "        graph_builder.add_node(\"router\", router)          # Sonnet 4\n",
    "        graph_builder.add_node(\"query_transform\", query_transform)  # ì¿¼ë¦¬ ë³€í™˜ ë…¸ë“œ ì¶”ê°€\n",
    "        graph_builder.add_node(\"retrieve\", retrieve_documents)\n",
    "        graph_builder.add_node(\"document_qa\", document_qa)    # Sonnet 4\n",
    "        graph_builder.add_node(\"general_qa\", general_qa)      # Sonnet 4 + ì›¹ íˆ´\n",
    "        graph_builder.add_node(\"tools\", tool_node)\n",
    "        \n",
    "        # ê¸°ë³¸ ì—£ì§€ë“¤\n",
    "        graph_builder.add_edge(START, \"router\")\n",
    "        \n",
    "        # ë¼ìš°í„°ì—ì„œ ë¶„ê¸°\n",
    "        graph_builder.add_conditional_edges(\n",
    "            \"router\",\n",
    "            route_by_category,\n",
    "            {\n",
    "                \"document_qa\": \"query_transform\",  # document â†’ query_transform â†’ retrieve â†’ document_qa\n",
    "                \"general_qa\": \"general_qa\",     # general â†’ general_qa\n",
    "            },\n",
    "        )\n",
    "        \n",
    "        # Document ê²½ë¡œ: query_transform â†’ retrieve â†’ document_qa â†’ END (Sonnet 4)\n",
    "        graph_builder.add_edge(\"query_transform\", \"retrieve\")\n",
    "        graph_builder.add_edge(\"retrieve\", \"document_qa\")\n",
    "        graph_builder.add_edge(\"document_qa\", END)\n",
    "        \n",
    "        # General ê²½ë¡œ: general_qa â†’ (í•„ìš”ì‹œ ì›¹ íˆ´) â†’ END (Sonnet 4)\n",
    "        graph_builder.add_conditional_edges(\n",
    "            \"general_qa\", \n",
    "            tools_condition,\n",
    "            {\n",
    "                \"tools\": \"tools\",     # ì›¹ íˆ´ í˜¸ì¶œ ì‹œ\n",
    "                \"__end__\": END        # íˆ´ í˜¸ì¶œ ì—†ìœ¼ë©´ ì¢…ë£Œ\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # ì›¹ íˆ´ ì‹¤í–‰ í›„ ë‹¤ì‹œ general_qaë¡œ ëŒì•„ê°€ì„œ ìµœì¢… ì‘ë‹µ\n",
    "        graph_builder.add_edge(\"tools\", \"general_qa\")\n",
    "        \n",
    "        # 4. ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "        graph = graph_builder.compile().with_config(\n",
    "            config={\"callbacks\": [langfuse_handler]}\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… Sonnet 4 ë‹¨ì¼ ëª¨ë¸ ì‹œìŠ¤í…œ ê·¸ë˜í”„ êµ¬ì„± ì™„ë£Œ!\")\n",
    "        \n",
    "        # 5. ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”\n",
    "        global conversation_history\n",
    "        conversation_history = []\n",
    "        print(\"âœ… ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™” ì™„ë£Œ!\")\n",
    "        \n",
    "        print(\"\\nğŸ“‹ ê·¸ë˜í”„ êµ¬ì¡°:\")\n",
    "        print(\"ğŸš€ ë¼ìš°í„°: Claude Sonnet 4 - ì¿¼ë¦¬ ë¶„ë¥˜\")\n",
    "        print(\"ğŸ“„ Document ì§ˆë¬¸: router â†’ query_transform â†’ retrieve â†’ document_qa â†’ END (Sonnet 4)\")\n",
    "        print(\"ğŸ’­ General ì§ˆë¬¸: router â†’ general_qa â†’ (í•„ìš”ì‹œ ì›¹ íˆ´) â†’ END (Sonnet 4)\")\n",
    "        \n",
    "        print(\"\\nğŸ¯ ëª¨ë¸ ì‚¬ìš© í˜„í™©:\")\n",
    "        print(\"- ëª¨ë“  ì‘ì—…: Claude Sonnet 4 (ìµœê³  ì„±ëŠ¥)\")\n",
    "        print(\"- ì¿¼ë¦¬ ë¶„ë¥˜: Claude Sonnet 4\")\n",
    "        print(\"- ë¬¸ì„œ QA: Claude Sonnet 4\") \n",
    "        print(\"- ì¼ë°˜ QA + ì›¹ ê²€ìƒ‰: Claude Sonnet 4\")\n",
    "        \n",
    "        # 6. ê·¸ë˜í”„ ì‹œê°í™” ì‹œë„\n",
    "        try:\n",
    "            print(\"\\nê·¸ë˜í”„ ì‹œê°í™” ì‹œë„ ì¤‘...\")\n",
    "            from IPython.display import Image, display\n",
    "            display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "            print(\"ê·¸ë˜í”„ ì‹œê°í™” ì™„ë£Œ!\")\n",
    "        except Exception as e:\n",
    "            print(f\"ê·¸ë˜í”„ ì‹œê°í™” ì‹¤íŒ¨: {e}\")\n",
    "        \n",
    "        # 7. ì¸í„°ë™í‹°ë¸Œ ì±„íŒ… ì‹œì‘\n",
    "        interactive_chat()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "779f005d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì¤‘...\n",
      "ë¬¸ì„œ ë¡œë”© ì¤‘...\n",
      "ë¬¸ì„œ ë¡œë”© ì™„ë£Œ: 40 í˜ì´ì§€\n",
      "ë¬¸ì„œ ì²­í‚¹ ì¤‘...\n",
      "ì²­í‚¹ ì™„ë£Œ: 80 ì²­í¬\n",
      "ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...\n",
      "ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: mps\n",
      "ğŸ“ ì„ë² ë”© ì°¨ì› í™•ì¸ ì¤‘...\n",
      "âœ… ì„ë² ë”© ì°¨ì›: 1024\n",
      "ğŸ—„ï¸ PGVector ì—°ê²° ì„¤ì • ì¤‘...\n",
      "ğŸ”Œ PostgreSQL ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...\n",
      "âŒ PGVector ì—°ê²° ì‹¤íŒ¨: name 'psycopg2' is not defined\n",
      "ğŸ’¡ PostgreSQLì´ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”:\n",
      "  - brew services start postgresql@15\n",
      "  - ë˜ëŠ” docker run -d --name pgvector-db -p 5432:5432 pgvector/pgvector:pg16\n",
      "ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: name 'psycopg2' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/4n/tjmm0jpn36jdkkfvpw1g79zw0000gn/T/ipykernel_26308/4263156414.py\", line 124, in main\n",
      "    rag_components = initialize_rag_components()\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/4n/tjmm0jpn36jdkkfvpw1g79zw0000gn/T/ipykernel_26308/3533317175.py\", line 70, in initialize_rag_components\n",
      "    conn = psycopg2.connect(CONNECTION_STRING)\n",
      "           ^^^^^^^^\n",
      "NameError: name 'psycopg2' is not defined\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa1a8cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bf61c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
